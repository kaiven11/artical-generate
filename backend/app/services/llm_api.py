"""
Large Language Model API service integration.
"""

import asyncio
import logging
import json
import traceback
from typing import Dict, Optional, Any, List
from dataclasses import dataclass
from datetime import datetime
import aiohttp


@dataclass
class LLMResponse:
    """LLM API response."""
    content: str
    success: bool = True
    error: Optional[str] = None
    usage: Optional[Dict[str, Any]] = None
    model: Optional[str] = None
    finish_reason: Optional[str] = None


class LLMAPIService:
    """Large Language Model API service."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.base_url = "http://localhost:8000/v1/chat/completions"
        self.api_key = "sk-dummy-f4689c69ad5746a8bb5b5e897b4033c7"
        self.timeout = aiohttp.ClientTimeout(total=300, connect=30, sock_read=60)  # 5 minutes total, 30s connect, 60s read
        self.default_model = "Claude-4-Sonnet"  # ä½¿ç”¨Claude-4-Sonnetæ¨¡å‹
        
    async def translate_content(
        self, 
        content: str, 
        title: str = "", 
        source_language: str = "en", 
        target_language: str = "zh",
        custom_prompt: str = ""
    ) -> LLMResponse:
        """
        Translate article content.
        
        Args:
            content: Content to translate
            title: Article title
            source_language: Source language code
            target_language: Target language code
            custom_prompt: Custom translation prompt
            
        Returns:
            LLMResponse with translated content
        """
        try:
            self.logger.info(f"Starting content translation from {source_language} to {target_language}")
            
            # Build translation prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self._build_translation_prompt(content, title, source_language, target_language)
            
            # Call LLM API
            response = await self._call_api(prompt)
            
            if response.success:
                self.logger.info("Content translation completed successfully")
            else:
                self.logger.error(f"Content translation failed: {response.error}")
            
            return response
            
        except Exception as e:
            self.logger.error(f"Translation error: {e}")
            return LLMResponse(
                content="",
                success=False,
                error=str(e)
            )
    
    async def optimize_content(
        self, 
        content: str, 
        title: str = "", 
        platform: str = "toutiao",
        optimization_type: str = "standard",
        custom_prompt: str = ""
    ) -> LLMResponse:
        """
        Optimize article content for specific platform.
        
        Args:
            content: Content to optimize
            title: Article title
            platform: Target platform (toutiao, weixin, etc.)
            optimization_type: Type of optimization (standard, seo, engagement)
            custom_prompt: Custom optimization prompt
            
        Returns:
            LLMResponse with optimized content
        """
        try:
            self.logger.info(f"Starting content optimization for platform: {platform}")
            
            # Build optimization prompt
            if custom_prompt:
                prompt = custom_prompt
            else:
                prompt = self._build_optimization_prompt(content, title, platform, optimization_type)
            
            # Call LLM API
            response = await self._call_api(prompt)
            
            if response.success:
                self.logger.info("Content optimization completed successfully")
            else:
                self.logger.error(f"Content optimization failed: {response.error}")
            
            return response
            
        except Exception as e:
            self.logger.error(f"Optimization error: {e}")
            return LLMResponse(
                content="",
                success=False,
                error=str(e)
            )

    async def create_content_by_topic(
        self,
        topic: str,
        keywords: list = None,
        requirements: str = "",
        custom_prompt: str = None,
        model: str = None,
        target_length: str = "mini",
        **api_params
    ) -> LLMResponse:
        """Create content based on topic using LLM API."""
        try:
            self.logger.info(f"ğŸ¨ å¼€å§‹ä¸»é¢˜å†…å®¹åˆ›ä½œ: {topic}")
            self.logger.info(f"ğŸ“ ç›®æ ‡é•¿åº¦: {target_length}")

            # å®šä¹‰ç›®æ ‡é•¿åº¦æ˜ å°„
            length_mapping = {
                "mini": {"words": "300-500", "description": "ç®€çŸ­æ–‡ç« "},
                "short": {"words": "500-800", "description": "çŸ­ç¯‡æ–‡ç« "},
                "medium": {"words": "800-1500", "description": "ä¸­ç­‰é•¿åº¦æ–‡ç« "},
                "long": {"words": "1500-3000", "description": "é•¿ç¯‡æ–‡ç« "}
            }

            length_info = length_mapping.get(target_length, length_mapping["mini"])
            self.logger.info(f"ğŸ“Š å­—æ•°è¦æ±‚: {length_info['words']} å­— ({length_info['description']})")

            if custom_prompt:
                # å¤„ç†è‡ªå®šä¹‰æç¤ºè¯ï¼Œæ·»åŠ é•¿åº¦è¦æ±‚
                self.logger.info("ğŸ“ ä½¿ç”¨è‡ªå®šä¹‰åˆ›ä½œæç¤ºè¯")

                # æ£€æŸ¥æ˜¯å¦åŒ…å«é•¿åº¦å˜é‡å ä½ç¬¦
                if "{target_length}" in custom_prompt:
                    prompt = custom_prompt.format(target_length=length_info['words'])
                    self.logger.info("âœ… å·²å¡«å……ç›®æ ‡é•¿åº¦å˜é‡")
                elif "å­—æ•°" not in custom_prompt and "é•¿åº¦" not in custom_prompt:
                    # å¦‚æœè‡ªå®šä¹‰æç¤ºè¯æ²¡æœ‰é•¿åº¦è¦æ±‚ï¼Œæ·»åŠ é•¿åº¦æŒ‡å¯¼
                    prompt = custom_prompt + f"\n\n**å­—æ•°è¦æ±‚ï¼š** è¯·æ§åˆ¶æ–‡ç« å­—æ•°åœ¨ {length_info['words']} å­—ä¹‹é—´ã€‚"
                    self.logger.info("âœ… å·²æ·»åŠ å­—æ•°è¦æ±‚åˆ°è‡ªå®šä¹‰æç¤ºè¯")
                else:
                    prompt = custom_prompt
                    self.logger.info("â„¹ï¸  è‡ªå®šä¹‰æç¤ºè¯å·²åŒ…å«é•¿åº¦è¦æ±‚")
            else:
                # Build default creation prompt with target length
                self.logger.info("ğŸ“ ä½¿ç”¨é»˜è®¤åˆ›ä½œæç¤ºè¯")
                prompt_parts = [
                    "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„æ–‡ç« ï¼š",
                    "",
                    f"ä¸»é¢˜ï¼š{topic}",
                ]

                if keywords:
                    prompt_parts.append(f"å…³é”®è¯ï¼š{', '.join(keywords)}")

                if requirements:
                    prompt_parts.append(f"åˆ›ä½œè¦æ±‚ï¼š{requirements}")

                # æ·»åŠ å­—æ•°è¦æ±‚
                prompt_parts.append(f"å­—æ•°è¦æ±‚ï¼š{length_info['words']} å­—")

                prompt_parts.extend([
                    "",
                    "è¯·ç¡®ä¿æ–‡ç« ï¼š",
                    "1. å†…å®¹åŸåˆ›ä¸”æœ‰æ·±åº¦",
                    "2. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º",
                    "3. è¯­è¨€æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯",
                    "4. åŒ…å«å®ç”¨ä»·å€¼å’Œè§è§£",
                    f"5. å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ {length_info['words']} å­—ä¹‹é—´",
                    "",
                    "è¯·ç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹ï¼Œä¸éœ€è¦é¢å¤–çš„è¯´æ˜ã€‚"
                ])

                prompt = "\n".join(prompt_parts)

            # Call LLM API with configurable parameters
            result = await self._call_api(prompt, model, **api_params)

            if result.success:
                self.logger.info("âœ… ä¸»é¢˜å†…å®¹åˆ›ä½œæˆåŠŸ")
                self.logger.info(f"ğŸ“ åˆ›ä½œå†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")

                # Try to extract title from content if possible
                lines = result.content.strip().split('\n')
                potential_title = lines[0].strip() if lines else ""

                # Simple heuristic to detect if first line is a title
                if (len(potential_title) < 100 and
                    not potential_title.endswith('ã€‚') and
                    not potential_title.endswith('.') and
                    len(lines) > 1):
                    result.title = potential_title
                    result.content = '\n'.join(lines[1:]).strip()
                    self.logger.info(f"ğŸ“° æå–åˆ°æ ‡é¢˜: {potential_title}")

            return result

        except Exception as e:
            self.logger.error(f"ğŸ’¥ ä¸»é¢˜å†…å®¹åˆ›ä½œå¼‚å¸¸: {str(e)}")
            return LLMResponse(
                success=False,
                content="",
                message=f"ä¸»é¢˜å†…å®¹åˆ›ä½œå¤±è´¥: {str(e)}"
            )

    async def _call_api(self, prompt: str, model: str = None, **kwargs) -> LLMResponse:
        """Call the LLM API with improved error handling and configurable parameters."""
        try:
            self.logger.info(f"ğŸš€ å¼€å§‹è°ƒç”¨LLM API...")
            self.logger.info(f"ğŸ”— APIåœ°å€: {self.base_url}")
            self.logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {model or self.default_model}")

            # å…¨é‡æ˜¾ç¤ºå‘é€ç»™APIçš„æç¤ºè¯å†…å®¹
            self.logger.info("ğŸ“¤ å‘é€ç»™LLMçš„å®Œæ•´æç¤ºè¯å†…å®¹:")
            self.logger.info("=" * 80)
            self.logger.info(prompt)
            self.logger.info("=" * 80)

            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json"
            }

            # å¯é…ç½®çš„APIå‚æ•°ï¼Œæ”¯æŒé€šè¿‡kwargsä¼ é€’
            # Claudeçš„æœ€å¤§tokensé™åˆ¶çº¦ä¸º200kï¼Œè®¾ç½®ä¸ºè¾ƒå¤§å€¼ä»¥é¿å…æˆªæ–­
            default_max_tokens = 100000  # ä½¿ç”¨è¾ƒå¤§çš„é»˜è®¤å€¼

            payload = {
                "model": model or self.default_model,
                "messages": [
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": kwargs.get('temperature', 0.7),
                "max_tokens": kwargs.get('max_tokens', default_max_tokens),
                "top_p": kwargs.get('top_p', 1.0),
                "frequency_penalty": kwargs.get('frequency_penalty', 0.0),
                "presence_penalty": kwargs.get('presence_penalty', 0.0)
            }

            self.logger.info(f"ğŸ“Š è¯·æ±‚å‚æ•°:")
            self.logger.info(f"   ğŸŒ¡ï¸  æ¸©åº¦: {payload['temperature']}")
            self.logger.info(f"   ğŸ“ æœ€å¤§tokens: {payload['max_tokens']}")
            self.logger.info(f"   ğŸ¯ top_p: {payload['top_p']}")
            self.logger.info(f"   ğŸ”„ frequency_penalty: {payload['frequency_penalty']}")
            self.logger.info(f"   ğŸ‘¥ presence_penalty: {payload['presence_penalty']}")

            self.logger.info(f"ğŸ“¦ è¯·æ±‚è½½è·å¤§å°: {len(str(payload))} å­—ç¬¦")

            # Use connector with proper settings to avoid connection issues
            connector = aiohttp.TCPConnector(
                limit=10,
                limit_per_host=5,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )

            async with aiohttp.ClientSession(
                timeout=self.timeout,
                connector=connector,
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"}
            ) as session:
                self.logger.info("ğŸŒ æ­£åœ¨å‘é€APIè¯·æ±‚...")
                async with session.post(self.base_url, headers=headers, json=payload) as response:
                    self.logger.info(f"ğŸ“¡ æ”¶åˆ°å“åº”ï¼ŒçŠ¶æ€ç : {response.status}")

                    if response.status == 200:
                        # Check content type to determine how to parse response
                        content_type = response.headers.get('content-type', '')
                        self.logger.info(f"ğŸ“‹ å“åº”å†…å®¹ç±»å‹: {content_type}")

                        if 'text/event-stream' in content_type:
                            # Handle Server-Sent Events (SSE) streaming response
                            self.logger.info("ğŸŒŠ æ£€æµ‹åˆ°æµå¼å“åº”ï¼Œå¼€å§‹å¤„ç†SSEæ•°æ®...")
                            content = await self._parse_sse_response(response)
                            return LLMResponse(
                                content=content,
                                success=True,
                                error=""
                            )
                        else:
                            # Handle standard JSON response
                            data = await response.json()
                            self.logger.info("âœ… APIè°ƒç”¨æˆåŠŸï¼Œæ­£åœ¨è§£æJSONå“åº”...")
                            return self._parse_api_response(data)
                    else:
                        error_text = await response.text()
                        self.logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        self.logger.error(f"ğŸ’¬ é”™è¯¯è¯¦æƒ…: {error_text}")
                        return LLMResponse(
                            content="",
                            success=False,
                            error=f"API request failed: {response.status} - {error_text}"
                        )

        except asyncio.CancelledError:
            self.logger.error("âš ï¸ APIè°ƒç”¨è¢«å–æ¶ˆ")
            return LLMResponse(
                content="",
                success=False,
                error="API call was cancelled"
            )
        except asyncio.TimeoutError:
            self.logger.error("â° APIè°ƒç”¨è¶…æ—¶")
            return LLMResponse(
                content="",
                success=False,
                error="API call timed out"
            )
        except Exception as e:
            self.logger.error(f"ğŸ’¥ APIè°ƒç”¨å¼‚å¸¸: {e}")
            import traceback
            self.logger.error(f"ğŸ’¥ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            return LLMResponse(
                content="",
                success=False,
                error=str(e)
            )

    async def _parse_sse_response(self, response) -> str:
        """Parse Server-Sent Events (SSE) streaming response."""
        try:
            self.logger.info("ğŸ”„ å¼€å§‹è§£æSSEæµå¼å“åº”...")
            content_parts = []

            async for line in response.content:
                line_str = line.decode('utf-8').strip()

                if line_str.startswith('data: '):
                    data_str = line_str[6:]  # Remove 'data: ' prefix

                    if data_str == '[DONE]':
                        self.logger.info("âœ… SSEæµç»“æŸ")
                        break

                    try:
                        import json
                        data = json.loads(data_str)

                        # Extract content from the streaming response
                        if 'choices' in data and len(data['choices']) > 0:
                            choice = data['choices'][0]
                            if 'delta' in choice and 'content' in choice['delta']:
                                content_chunk = choice['delta']['content']
                                content_parts.append(content_chunk)
                                self.logger.debug(f"ğŸ“ æ”¶åˆ°å†…å®¹å—: {content_chunk[:50]}...")
                            elif 'message' in choice and 'content' in choice['message']:
                                # Handle non-streaming format
                                content_chunk = choice['message']['content']
                                content_parts.append(content_chunk)
                                self.logger.debug(f"ğŸ“ æ”¶åˆ°å®Œæ•´å†…å®¹: {content_chunk[:50]}...")

                    except json.JSONDecodeError as e:
                        self.logger.debug(f"âš ï¸ è·³è¿‡éJSONè¡Œ: {line_str[:100]}...")
                        continue

            full_content = ''.join(content_parts)
            self.logger.info(f"âœ… SSEè§£æå®Œæˆï¼Œæ€»å†…å®¹é•¿åº¦: {len(full_content)} å­—ç¬¦")
            return full_content

        except Exception as e:
            self.logger.error(f"ğŸ’¥ SSEè§£æå¼‚å¸¸: {e}")
            # Fallback: try to read as plain text
            try:
                text_content = await response.text()
                self.logger.info("ğŸ”„ å°è¯•ä½œä¸ºçº¯æ–‡æœ¬è¯»å–...")
                return text_content
            except Exception as fallback_e:
                self.logger.error(f"ğŸ’¥ çº¯æ–‡æœ¬è¯»å–ä¹Ÿå¤±è´¥: {fallback_e}")
                return ""

    def _parse_api_response(self, data: Dict[str, Any]) -> LLMResponse:
        """Parse API response."""
        try:
            choices = data.get("choices", [])
            if not choices:
                return LLMResponse(
                    content="",
                    success=False,
                    error="No choices in API response"
                )
            
            choice = choices[0]
            message = choice.get("message", {})
            content = message.get("content", "").strip()
            
            usage = data.get("usage", {})
            model = data.get("model", "")
            finish_reason = choice.get("finish_reason", "")
            
            return LLMResponse(
                content=content,
                success=True,
                usage=usage,
                model=model,
                finish_reason=finish_reason
            )
            
        except Exception as e:
            self.logger.error(f"Error parsing API response: {e}")
            return LLMResponse(
                content="",
                success=False,
                error=f"Error parsing response: {str(e)}"
            )
    
    def _build_translation_prompt(
        self, 
        content: str, 
        title: str, 
        source_lang: str, 
        target_lang: str
    ) -> str:
        """Build translation prompt."""
        lang_names = {
            "en": "è‹±æ–‡",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æ–‡",
            "ko": "éŸ©æ–‡"
        }
        
        source_name = lang_names.get(source_lang, source_lang)
        target_name = lang_names.get(target_lang, target_lang)
        
        prompt = f"""è¯·å°†ä»¥ä¸‹{source_name}æ–‡ç« ç¿»è¯‘æˆ{target_name}ï¼Œè¦æ±‚ï¼š

1. ä¿æŒåŸæ–‡çš„è¯­ä¹‰å’Œè¯­è°ƒ
2. ä½¿ç”¨è‡ªç„¶æµç•…çš„{target_name}è¡¨è¾¾
3. ä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§
4. é€‚å½“è°ƒæ•´å¥å¼ä»¥ç¬¦åˆ{target_name}é˜…è¯»ä¹ æƒ¯
5. ä¿æŒæ®µè½ç»“æ„ä¸å˜

"""
        
        if title:
            prompt += f"æ–‡ç« æ ‡é¢˜ï¼š{title}\n\n"
        
        prompt += f"æ–‡ç« å†…å®¹ï¼š\n{content}\n\nè¯·ç›´æ¥è¾“å‡ºç¿»è¯‘ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚"
        
        return prompt
    
    def _build_optimization_prompt(
        self, 
        content: str, 
        title: str, 
        platform: str, 
        optimization_type: str
    ) -> str:
        """Build content optimization prompt."""
        platform_rules = {
            "toutiao": "ä»Šæ—¥å¤´æ¡å¹³å°ç‰¹ç‚¹ï¼šæ ‡é¢˜è¦å¸å¼•çœ¼çƒï¼Œå†…å®¹è¦æœ‰äº‰è®®æ€§å’Œè¯é¢˜æ€§ï¼Œé€‚åˆå¤§ä¼—é˜…è¯»",
            "weixin": "å¾®ä¿¡å…¬ä¼—å·å¹³å°ç‰¹ç‚¹ï¼šå†…å®¹è¦æœ‰ä»·å€¼ï¼Œæ ‡é¢˜è¦ç²¾å‡†ï¼Œé€‚åˆåˆ†äº«ä¼ æ’­",
            "zhihu": "çŸ¥ä¹å¹³å°ç‰¹ç‚¹ï¼šå†…å®¹è¦ä¸“ä¸šæ·±å…¥ï¼Œé€»è¾‘æ¸…æ™°ï¼Œæœ‰çŸ¥è¯†ä»·å€¼",
            "xiaohongshu": "å°çº¢ä¹¦å¹³å°ç‰¹ç‚¹ï¼šå†…å®¹è¦ç”Ÿæ´»åŒ–ï¼Œæœ‰å®ç”¨ä»·å€¼ï¼Œé€‚åˆå¹´è½»ç”¨æˆ·"
        }
        
        optimization_rules = {
            "standard": "æ ‡å‡†ä¼˜åŒ–ï¼šæå‡å¯è¯»æ€§ï¼Œä¼˜åŒ–ç»“æ„ï¼Œå¢å¼ºå¸å¼•åŠ›",
            "seo": "SEOä¼˜åŒ–ï¼šå¢åŠ å…³é”®è¯å¯†åº¦ï¼Œä¼˜åŒ–æ ‡é¢˜å’Œæ®µè½ç»“æ„",
            "engagement": "äº’åŠ¨ä¼˜åŒ–ï¼šå¢åŠ äº’åŠ¨å…ƒç´ ï¼Œæå‡ç”¨æˆ·å‚ä¸åº¦"
        }
        
        platform_rule = platform_rules.get(platform, "é€šç”¨å¹³å°ä¼˜åŒ–")
        optimization_rule = optimization_rules.get(optimization_type, "æ ‡å‡†ä¼˜åŒ–")
        
        prompt = f"""è¯·å¯¹ä»¥ä¸‹æ–‡ç« å†…å®¹è¿›è¡Œä¼˜åŒ–ï¼Œè¦æ±‚ï¼š

å¹³å°ç‰¹ç‚¹ï¼š{platform_rule}
ä¼˜åŒ–ç±»å‹ï¼š{optimization_rule}

ä¼˜åŒ–è¦æ±‚ï¼š
1. ä¿æŒåŸæ–‡æ ¸å¿ƒè§‚ç‚¹å’Œä¿¡æ¯ä¸å˜
2. ä¼˜åŒ–è¯­è¨€è¡¨è¾¾ï¼Œä½¿å…¶æ›´ç¬¦åˆå¹³å°ç‰¹è‰²
3. è°ƒæ•´æ®µè½ç»“æ„ï¼Œæå‡é˜…è¯»ä½“éªŒ
4. å¢åŠ é€‚å½“çš„è¿‡æ¸¡è¯å’Œè¿æ¥è¯
5. ç¡®ä¿å†…å®¹åŸåˆ›æ€§ï¼Œé¿å…AIç—•è¿¹è¿‡é‡

"""
        
        if title:
            prompt += f"åŸæ ‡é¢˜ï¼š{title}\n\n"
        
        prompt += f"åŸæ–‡å†…å®¹ï¼š\n{content}\n\nè¯·ç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚"
        
        return prompt


# Service instance
_llm_service = None

def get_llm_service() -> LLMAPIService:
    """Get LLM service instance."""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMAPIService()
    return _llm_service
