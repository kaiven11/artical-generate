"""
Content extraction service using Freedium.cfd for Medium articles.
"""

import asyncio
import logging
import re
import time
import os
from typing import Dict, Optional, Any
from urllib.parse import urlparse, urljoin
import aiohttp
from bs4 import BeautifulSoup
from dataclasses import dataclass

from ..core.performance_config import get_performance_config
from datetime import datetime

try:
    from DrissionPage import ChromiumPage, ChromiumOptions
    DRISSION_AVAILABLE = True
except ImportError:
    DRISSION_AVAILABLE = False
    ChromiumPage = None
    ChromiumOptions = None


@dataclass
class ExtractedContent:
    """Extracted article content."""
    title: str
    content: str
    author: Optional[str] = None
    publish_date: Optional[str] = None
    tags: list = None
    summary: Optional[str] = None
    word_count: int = 0
    reading_time: int = 0
    source_url: str = ""
    extraction_method: str = ""
    success: bool = True
    error: Optional[str] = None


class ContentExtractor:
    """Content extraction service."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.freedium_base = "https://freedium.cfd"

        # ä½¿ç”¨æ€§èƒ½é…ç½®
        self.perf_config = get_performance_config()
        extraction_config = self.perf_config.get_content_extraction_config()

        self.timeout = aiohttp.ClientTimeout(total=extraction_config["http_timeout"])
        
    async def extract_content(self, url: str) -> ExtractedContent:
        """
        Extract content from article URL.

        Args:
            url: Article URL to extract content from

        Returns:
            ExtractedContent with extracted article data
        """
        self.logger.info("="*80)
        self.logger.info(f"ğŸš€ å¼€å§‹å†…å®¹æå–æµç¨‹: {url}")
        self.logger.info(f"ğŸ”§ DrissionPageå¯ç”¨æ€§: {DRISSION_AVAILABLE}")
        self.logger.info("="*80)

        # Try different extraction methods
        methods = [
            ("freedium", self._extract_via_freedium),
            ("direct", self._extract_direct),
        ]

        for method_name, method_func in methods:
            try:
                self.logger.info(f"ğŸ”„ å°è¯•æå–æ–¹æ³•: {method_name}")
                self.logger.info(f"ğŸ“‹ å³å°†è°ƒç”¨æ–¹æ³•: {method_func.__name__}")

                result = await method_func(url)

                self.logger.info(f"ğŸ“Š æ–¹æ³• {method_name} æ‰§è¡Œå®Œæˆï¼ŒæˆåŠŸ: {result.success}")
                if result.success and result.content:
                    result.extraction_method = method_name
                    self.logger.info(f"âœ… ä½¿ç”¨ {method_name} æ–¹æ³•æˆåŠŸæå–å†…å®¹")
                    return result
                else:
                    self.logger.warning(f"âš ï¸ æ–¹æ³• {method_name} æœªèƒ½æå–åˆ°æœ‰æ•ˆå†…å®¹")

            except Exception as e:
                self.logger.error(f"ğŸ’¥ æå–æ–¹æ³• {method_name} å¤±è´¥: {e}")
                import traceback
                self.logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                continue
        
        # All methods failed
        return ExtractedContent(
            title="",
            content="",
            source_url=url,
            success=False,
            error="All extraction methods failed"
        )
    
    async def _extract_via_freedium(self, url: str) -> ExtractedContent:
        """Extract content using Freedium.cfd service with visual browser."""
        if not DRISSION_AVAILABLE:
            raise RuntimeError("DrissionPage not installed. Please install: pip install DrissionPage")

        # Ensure URL is properly formatted for Freedium
        # Freedium expects: https://freedium.cfd/https://medium.com/...
        if not url.startswith('http'):
            url = f"https://{url}"

        # Construct Freedium URL correctly (no + prefix needed)
        freedium_url = f"{self.freedium_base}/{url}"
        self.logger.info(f"ğŸ”— åŸå§‹URL: {url}")
        self.logger.info(f"ğŸŒ ä½¿ç”¨å¯è§†åŒ–æµè§ˆå™¨è®¿é—® Freedium: {freedium_url}")
        
        page = None
        try:
            # åˆ›å»ºæµè§ˆå™¨é…ç½®
            options = self._create_browser_options()
            
            # åˆ›å»ºæµè§ˆå™¨å®ä¾‹
            self.logger.info("ğŸš€ å¯åŠ¨Chromeæµè§ˆå™¨è¿›è¡Œå†…å®¹æå–...")
            self.logger.info("ğŸ“º æµè§ˆå™¨å°†ä»¥å¯è§†åŒ–æ¨¡å¼è¿è¡Œï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ•´ä¸ªæå–è¿‡ç¨‹")
            self.logger.info("ğŸ”¥ æµè§ˆå™¨çª—å£å³å°†å¼¹å‡ºï¼Œè¯·æ³¨æ„è§‚å¯Ÿï¼")
            self.logger.info("ğŸ“‹ æ­£åœ¨åˆ›å»º ChromiumPage å®ä¾‹...")

            try:
                page = ChromiumPage(addr_or_opts=options)
                self.logger.info("âœ… ChromiumPage å®ä¾‹åˆ›å»ºæˆåŠŸ!")
            except Exception as e:
                self.logger.error(f"ğŸ’¥ ChromiumPage åˆ›å»ºå¤±è´¥: {e}")
                import traceback
                self.logger.error(f"ğŸ“‹ é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
                raise

            # ç¡®ä¿çª—å£æ¿€æ´»å¹¶ç½®äºå‰å°
            self.logger.info("ğŸ”§ æ­£åœ¨é…ç½®æµè§ˆå™¨çª—å£...")
            try:
                page.set.window.max()  # æœ€å¤§åŒ–çª—å£
                self.logger.info("âœ… çª—å£æœ€å¤§åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"âš ï¸ çª—å£æœ€å¤§åŒ–å¤±è´¥: {e}")

            try:
                # å°è¯•ç½®äºå‰å°ï¼ˆå¦‚æœæ–¹æ³•å­˜åœ¨ï¼‰
                if hasattr(page.set.window, 'to_front'):
                    page.set.window.to_front()
                    self.logger.info("âœ… çª—å£ç½®äºå‰å°æˆåŠŸ")
                else:
                    self.logger.info("â„¹ï¸ çª—å£ç½®äºå‰å°æ–¹æ³•ä¸å¯ç”¨ï¼Œè·³è¿‡")
            except Exception as e:
                self.logger.warning(f"âš ï¸ çª—å£ç½®äºå‰å°å¤±è´¥: {e}")

            self.logger.info("âœ… Chromeæµè§ˆå™¨å¯åŠ¨æˆåŠŸ!")
            self.logger.info("ğŸŒ æµè§ˆå™¨çª—å£å·²æ‰“å¼€ï¼Œæ­£åœ¨è®¿é—®Freedium...")
            self.logger.info("ğŸ‘ï¸ è¯·æŸ¥çœ‹å¼¹å‡ºçš„æµè§ˆå™¨çª—å£ï¼")
            
            # ç»™ç”¨æˆ·æ—¶é—´è§‚å¯Ÿæµè§ˆå™¨çª—å£ - ä½¿ç”¨æ€§èƒ½é…ç½®
            extraction_config = self.perf_config.get_content_extraction_config()
            startup_wait = extraction_config["browser_startup_wait"]
            self.logger.info(f"â³ ç­‰å¾…{startup_wait}ç§’ï¼Œè®©æ‚¨è§‚å¯Ÿæµè§ˆå™¨çª—å£...")
            time.sleep(startup_wait)

            # è®¿é—®Freediumé¡µé¢
            self.logger.info(f"ğŸ“„ æ­£åœ¨è®¿é—®é¡µé¢: {freedium_url}")
            self.logger.info("ğŸ‘€ è¯·è§‚å¯Ÿæµè§ˆå™¨çª—å£ä¸­çš„é¡µé¢åŠ è½½è¿‡ç¨‹...")
            page.get(freedium_url)

            # ç­‰å¾…é¡µé¢åŠ è½½ï¼Œç»™ç”¨æˆ·æ—¶é—´è§‚å¯Ÿ - ä½¿ç”¨æ€§èƒ½é…ç½®
            page_load_wait = extraction_config["page_load_wait"]
            self.logger.info("â³ ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ...")
            time.sleep(page_load_wait)
            
            # è·å–é¡µé¢æ ‡é¢˜
            page_title = page.title
            self.logger.info(f"ğŸ“ é¡µé¢æ ‡é¢˜: {page_title}")
            
            # ç­‰å¾…å†…å®¹åŠ è½½å®Œæˆ - ä½¿ç”¨æ€§èƒ½é…ç½®
            content_load_wait = extraction_config["content_load_wait"]
            self.logger.info("â³ ç­‰å¾…é¡µé¢å†…å®¹åŠ è½½...")
            time.sleep(content_load_wait)
            
            # è·å–é¡µé¢HTML
            html = page.html
            self.logger.info(f"ğŸ“Š é¡µé¢HTMLé•¿åº¦: {len(html)} å­—ç¬¦")
            
            # è§£æå†…å®¹
            result = self._parse_freedium_html(html, url)
            
            if result.success:
                self.logger.info("âœ… å†…å®¹æå–æˆåŠŸ!")
                self.logger.info(f"ğŸ“° æ–‡ç« æ ‡é¢˜: {result.title}")
                self.logger.info(f"ğŸ‘¤ ä½œè€…: {result.author or 'æœªçŸ¥'}")
                self.logger.info(f"ğŸ“ å†…å®¹é•¿åº¦: {result.word_count} è¯")
                self.logger.info(f"ğŸ“ å®é™…å†…å®¹é•¿åº¦: {len(result.content)} å­—ç¬¦")
                self.logger.info(f"â±ï¸ é¢„è®¡é˜…è¯»æ—¶é—´: {result.reading_time} åˆ†é’Ÿ")
                self.logger.info(f"ğŸ“„ å†…å®¹é¢„è§ˆ: {result.content[:200]}...")
            else:
                self.logger.warning("âš ï¸ å†…å®¹æå–ä¸å®Œæ•´ï¼Œå°è¯•å¢å¼ºæå–...")
                # å°è¯•æ›´ç§¯æçš„æå–æ–¹æ³•
                result = self._enhanced_content_extraction(page, url)
                
            return result
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ æµè§ˆå™¨å†…å®¹æå–å¤±è´¥: {e}")
            raise
        finally:
            if page:
                try:
                    self.logger.info("ğŸ”’ å…³é—­æµè§ˆå™¨")
                    page.quit()
                except:
                    pass
    
    def _create_browser_options(self):
        """åˆ›å»ºæµè§ˆå™¨é…ç½®é€‰é¡¹"""
        options = ChromiumOptions()
        
        # æŒ‡å®šChromeæµè§ˆå™¨è·¯å¾„ï¼ˆæŒ‡çº¹æµè§ˆå™¨ï¼‰
        chrome_path = r"C:\Users\asus\AppData\Local\Chromium\Application\chrome.exe"
        self.logger.info(f"ğŸ”§ ä½¿ç”¨æŒ‡çº¹æµè§ˆå™¨è·¯å¾„: {chrome_path}")
        options.set_browser_path(chrome_path)
        
        # ç”¨æˆ·æ•°æ®ç›®å½•
        current_dir = os.getcwd()
        user_data_dir = os.path.join(current_dir, "chro")
        os.makedirs(user_data_dir, exist_ok=True)
        self.logger.info(f"ğŸ“ ç”¨æˆ·æ•°æ®ç›®å½•: {user_data_dir}")
        options.set_user_data_path(user_data_dir)
        
        # æŒ‡çº¹å‚æ•°
        options.set_argument("--fingerprint=1000")
        options.set_argument("--fingerprint-platform=windows")
        options.set_argument("--fingerprint-brand=Chrome")
        options.set_argument("--lang=zh-CN")
        options.set_argument("--timezone=Asia/Shanghai")
        
        # åæ£€æµ‹é…ç½®
        options.set_argument("--disable-blink-features=AutomationControlled")
        options.set_argument("--disable-dev-shm-usage")
        options.set_argument("--no-sandbox")
        options.set_argument("--disable-web-security")
        options.set_argument("--disable-features=VizDisplayCompositor")
        
        # çª—å£é…ç½® - å¼ºåˆ¶å¯è§†åŒ–æ¨¡å¼ï¼Œç¡®ä¿ç”¨æˆ·èƒ½çœ‹åˆ°æµè§ˆå™¨æ“ä½œ
        options.set_argument("--window-size=1400,900")  # æ›´å¤§çš„çª—å£
        options.set_argument("--window-position=50,50")   # é è¿‘å±å¹•å·¦ä¸Šè§’
        options.set_argument("--start-maximized")         # å¯åŠ¨æ—¶æœ€å¤§åŒ–
        options.set_argument("--disable-web-security")    # ç¦ç”¨webå®‰å…¨é™åˆ¶
        options.set_argument("--disable-features=VizDisplayCompositor")
        options.set_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

        # å¼ºåˆ¶ç¦ç”¨æ— å¤´æ¨¡å¼ - ç¡®ä¿æµè§ˆå™¨çª—å£å¯è§
        # ç»å¯¹ä¸ä½¿ç”¨æ— å¤´æ¨¡å¼
        options.set_argument("--disable-headless")
        options.set_argument("--no-headless")

        # ç¡®ä¿çª—å£åœ¨å‰å°æ˜¾ç¤º
        options.set_argument("--force-device-scale-factor=1")
        options.set_argument("--high-dpi-support=1")
        
        self.logger.info("ğŸ”§ æµè§ˆå™¨é…ç½®: å¯è§†åŒ–æ¨¡å¼ï¼Œ1200x800çª—å£")
        return options
    
    async def _extract_direct(self, url: str) -> ExtractedContent:
        """Extract content directly from the original URL."""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        async with aiohttp.ClientSession(timeout=self.timeout, headers=headers) as session:
            async with session.get(url) as response:
                if response.status != 200:
                    raise Exception(f"Direct access returned status {response.status}")
                
                html = await response.text()
                return self._parse_medium_html(html, url)
    
    def _parse_freedium_html(self, html: str, source_url: str) -> ExtractedContent:
        """Parse HTML content from Freedium.cfd."""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Extract title
        title = ""
        title_selectors = ['h1', 'title', '[data-testid="storyTitle"]']
        for selector in title_selectors:
            title_elem = soup.select_one(selector)
            if title_elem:
                title = title_elem.get_text().strip()
                break
        
        # Extract main content
        content = ""
        content_selectors = [
            'article',
            '[data-testid="storyContent"]',
            '.story-content',
            'main',
            '.post-content'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # Remove unwanted elements
                for unwanted in content_elem.select('script, style, nav, header, footer, .ad, .advertisement'):
                    unwanted.decompose()
                
                content = content_elem.get_text().strip()
                break
        
        # Extract author
        author = ""
        author_selectors = [
            '[data-testid="authorName"]',
            '.author-name',
            '[rel="author"]',
            '.byline-author'
        ]
        
        for selector in author_selectors:
            author_elem = soup.select_one(selector)
            if author_elem:
                author = author_elem.get_text().strip()
                break
        
        # Extract publish date
        publish_date = ""
        date_selectors = [
            'time[datetime]',
            '[data-testid="storyPublishDate"]',
            '.publish-date'
        ]
        
        for selector in date_selectors:
            date_elem = soup.select_one(selector)
            if date_elem:
                publish_date = date_elem.get('datetime') or date_elem.get_text().strip()
                break
        
        # Calculate word count and reading time
        word_count = len(content.split()) if content else 0
        reading_time = max(1, word_count // 200)  # Assume 200 words per minute
        
        # Generate summary (first 200 characters)
        summary = content[:200] + "..." if len(content) > 200 else content
        
        return ExtractedContent(
            title=title,
            content=content,
            author=author,
            publish_date=publish_date,
            summary=summary,
            word_count=word_count,
            reading_time=reading_time,
            source_url=source_url,
            success=bool(title and content)
        )
    
    def _parse_medium_html(self, html: str, source_url: str) -> ExtractedContent:
        """Parse HTML content from Medium directly."""
        soup = BeautifulSoup(html, 'html.parser')
        
        # Medium-specific selectors
        title = ""
        title_elem = soup.select_one('h1[data-testid="storyTitle"], h1.graf--title')
        if title_elem:
            title = title_elem.get_text().strip()
        
        # Extract content paragraphs
        content_parts = []
        content_selectors = [
            'article section p',
            '.story-content p',
            '[data-testid="storyContent"] p'
        ]
        
        for selector in content_selectors:
            paragraphs = soup.select(selector)
            if paragraphs:
                content_parts = [p.get_text().strip() for p in paragraphs if p.get_text().strip()]
                break
        
        content = "\n\n".join(content_parts)
        
        # Extract author
        author = ""
        author_elem = soup.select_one('[data-testid="authorName"], .author-name')
        if author_elem:
            author = author_elem.get_text().strip()
        
        # Calculate metrics
        word_count = len(content.split()) if content else 0
        reading_time = max(1, word_count // 200)
        summary = content[:200] + "..." if len(content) > 200 else content
        
        return ExtractedContent(
            title=title,
            content=content,
            author=author,
            summary=summary,
            word_count=word_count,
            reading_time=reading_time,
            source_url=source_url,
            success=bool(title and content)
        )
    
    def _enhanced_content_extraction(self, page, url: str) -> ExtractedContent:
        """å¢å¼ºçš„å†…å®¹æå–æ–¹æ³•ï¼Œç›´æ¥ä»é¡µé¢DOMè·å–å†…å®¹"""
        try:
            self.logger.info("ğŸ” ä½¿ç”¨å¢å¼ºæå–æ–¹æ³•...")
            
            # è·å–é¡µé¢æ ‡é¢˜
            title = ""
            try:
                title_elem = page.ele('tag:h1')
                if title_elem:
                    title = title_elem.text.strip()
                    self.logger.info(f"ğŸ“° æå–åˆ°æ ‡é¢˜: {title}")
            except:
                pass
            
            # è·å–ä½œè€…ä¿¡æ¯
            author = ""
            try:
                # å°è¯•å¤šç§ä½œè€…é€‰æ‹©å™¨
                author_selectors = ['[data-testid="authorName"]', '.author', '.byline', 'a[rel="author"]']
                for selector in author_selectors:
                    try:
                        author_elem = page.ele(selector)
                        if author_elem:
                            author = author_elem.text.strip()
                            self.logger.info(f"ğŸ‘¤ æå–åˆ°ä½œè€…: {author}")
                            break
                    except:
                        continue
            except:
                pass
            
            # è·å–æ‰€æœ‰æ®µè½å†…å®¹
            content_parts = []
            try:
                # è·å–æ‰€æœ‰æ®µè½
                paragraphs = page.eles('tag:p')
                self.logger.info(f"ğŸ“ æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½")
                
                for i, p in enumerate(paragraphs):
                    try:
                        text = p.text.strip()
                        if text and len(text) > 20:  # è¿‡æ»¤å¤ªçŸ­çš„æ®µè½
                            content_parts.append(text)
                            if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ®µè½çš„é¢„è§ˆ
                                self.logger.info(f"ğŸ“„ æ®µè½ {i+1}: {text[:100]}...")
                    except:
                        continue
                        
            except Exception as e:
                self.logger.warning(f"æ®µè½æå–å¤±è´¥: {e}")
            
            # ç»„åˆå†…å®¹
            content = "\n\n".join(content_parts)

            # æ¸…ç†Freediumå¹³å°è‡ªå·±çš„å†…å®¹
            content = self._clean_freedium_content(content)

            if not content:
                # å°è¯•è·å–æ•´ä¸ªæ–‡ç« ä½“
                try:
                    article_elem = page.ele('tag:article')
                    if article_elem:
                        content = article_elem.text.strip()
                        content = self._clean_freedium_content(content)
                        self.logger.info("ğŸ“„ ä»articleæ ‡ç­¾è·å–å†…å®¹")
                except:
                    pass
            
            if not content:
                # æœ€åå°è¯•è·å–mainæ ‡ç­¾å†…å®¹
                try:
                    main_elem = page.ele('tag:main')
                    if main_elem:
                        content = main_elem.text.strip()
                        self.logger.info("ğŸ“„ ä»mainæ ‡ç­¾è·å–å†…å®¹")
                except:
                    pass
            
            # è®¡ç®—æŒ‡æ ‡
            word_count = len(content.split()) if content else 0
            reading_time = max(1, word_count // 200)
            summary = content[:200] + "..." if len(content) > 200 else content
            
            self.logger.info(f"âœ… å¢å¼ºæå–å®Œæˆ: æ ‡é¢˜={'æœ‰' if title else 'æ— '}, å†…å®¹={len(content)}å­—ç¬¦, {word_count}è¯")
            
            return ExtractedContent(
                title=title or "æœªæå–åˆ°æ ‡é¢˜",
                content=content,
                author=author or "æœªçŸ¥ä½œè€…",
                summary=summary,
                word_count=word_count,
                reading_time=reading_time,
                source_url=url,
                extraction_method="enhanced",
                success=bool(content)  # åªè¦æœ‰å†…å®¹å°±ç®—æˆåŠŸ
            )
            
        except Exception as e:
            self.logger.error(f"ğŸ’¥ å¢å¼ºæå–å¤±è´¥: {e}")
            return ExtractedContent(
                title="æå–å¤±è´¥",
                content="",
                source_url=url,
                success=False,
                error=str(e)
            )

    def _clean_freedium_content(self, content: str) -> str:
        """æ¸…ç†Freediumå¹³å°è‡ªå·±çš„å†…å®¹ï¼Œåªä¿ç•™åŸå§‹æ–‡ç« å†…å®¹"""
        if not content:
            return content

        # Freediumå¹³å°å†…å®¹çš„æ ‡è¯†
        freedium_markers = [
            "Dear Freedium users,",
            "We've updated our donation options",
            "Your contributions are invaluable",
            "Choose Your Preferred Donation Platform:",
            "Sometimes we have problems displaying some Medium posts",
            "If you have a problem that some images aren't loading",
            "try using VPN",
            "Cloudflare's bot detection algorithms"
        ]

        lines = content.split('\n')
        cleaned_lines = []
        skip_mode = False

        for line in lines:
            line_stripped = line.strip()

            # æ£€æŸ¥æ˜¯å¦æ˜¯Freediumå¹³å°å†…å®¹çš„å¼€å§‹
            if any(marker in line_stripped for marker in freedium_markers):
                skip_mode = True
                continue

            # å¦‚æœåœ¨è·³è¿‡æ¨¡å¼ä¸­ï¼Œæ£€æŸ¥æ˜¯å¦é‡åˆ°äº†çœŸæ­£çš„æ–‡ç« å†…å®¹
            if skip_mode:
                # å¦‚æœè¿™è¡Œçœ‹èµ·æ¥åƒæ˜¯æ–‡ç« å†…å®¹çš„å¼€å§‹ï¼ˆé•¿åº¦è¶³å¤Ÿä¸”ä¸æ˜¯å¹³å°ä¿¡æ¯ï¼‰
                if (len(line_stripped) > 50 and
                    not any(marker in line_stripped for marker in freedium_markers) and
                    not line_stripped.startswith('http') and
                    not 'donation' in line_stripped.lower() and
                    not 'support' in line_stripped.lower() and
                    not 'freedium' in line_stripped.lower()):
                    skip_mode = False
                    cleaned_lines.append(line)
                continue

            # æ­£å¸¸æ¨¡å¼ï¼Œä¿ç•™å†…å®¹
            if line_stripped:  # åªä¿ç•™éç©ºè¡Œ
                cleaned_lines.append(line)

        cleaned_content = '\n'.join(cleaned_lines).strip()

        # å¦‚æœæ¸…ç†åå†…å®¹å¤ªçŸ­ï¼Œè¿”å›åŸå†…å®¹
        if len(cleaned_content) < len(content) * 0.3:
            self.logger.warning("æ¸…ç†åå†…å®¹è¿‡çŸ­ï¼Œè¿”å›åŸå†…å®¹")
            return content

        self.logger.info(f"å†…å®¹æ¸…ç†å®Œæˆ: {len(content)} -> {len(cleaned_content)} å­—ç¬¦")
        return cleaned_content


# Service instance
_content_extractor = None

def get_content_extractor() -> ContentExtractor:
    """Get content extractor service instance."""
    global _content_extractor
    if _content_extractor is None:
        _content_extractor = ContentExtractor()
    return _content_extractor
