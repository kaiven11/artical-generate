# -*- coding: utf-8 -*-
"""
Article Processing Service
Handles the complete article processing pipeline including translation, optimization, and detection.
"""

import logging
import asyncio
from typing import List, Dict, Any, Optional
from datetime import datetime
from enum import Enum

from ..models.article import ArticleStatus
from ..models.task import TaskStatus
from ..core.database import get_db_session


# Simple data classes for processing
class Article:
    """Simple article data class for processing."""
    def __init__(self, id=None, title="", source_url="", source_platform="",
                 content_original="", content_translated=None, content_optimized=None,
                 content_final=None, status=ArticleStatus.PENDING, **kwargs):
        self.id = id
        self.title = title
        self.source_url = source_url
        self.source_platform = source_platform
        self.content_original = content_original
        self.content_translated = content_translated
        self.content_optimized = content_optimized
        self.content_final = content_final
        self.status = status
        # Store any additional attributes
        for key, value in kwargs.items():
            setattr(self, key, value)


class Task:
    """Simple task data class for processing."""
    def __init__(self, id=None, task_id="", name="", type="article_processing",
                 status=TaskStatus.PENDING, article_id=None, **kwargs):
        self.id = id
        self.task_id = task_id
        self.name = name
        self.type = type
        self.status = status
        self.article_id = article_id
        # Store any additional attributes
        for key, value in kwargs.items():
            setattr(self, key, value)

logger = logging.getLogger(__name__)


class ProcessingStep(str, Enum):
    """Article processing steps."""
    CREATE = "create"      # New step for topic-based content creation
    EXTRACT = "extract"
    TRANSLATE = "translate"
    OPTIMIZE = "optimize"
    DETECT = "detect"
    PUBLISH = "publish"


class ProcessingResult:
    """Result of a processing step."""
    
    def __init__(self, success: bool, message: str = "", data: Dict[str, Any] = None):
        self.success = success
        self.message = message
        self.data = data or {}
        self.timestamp = datetime.utcnow()


class ArticleProcessor:
    """Main article processing service."""
    
    def __init__(self):
        self.logger = logger
        self.processing_steps = {
            ProcessingStep.CREATE: self._create_content,
            ProcessingStep.EXTRACT: self._extract_content,
            ProcessingStep.TRANSLATE: self._translate_content,
            ProcessingStep.OPTIMIZE: self._optimize_content,
            ProcessingStep.DETECT: self._detect_content,
            ProcessingStep.PUBLISH: self._publish_content
        }
    
    async def process_article(
        self,
        article_id: int,
        steps: List[str] = None,
        auto_publish: bool = False,
        priority: str = "normal"
    ) -> Dict[str, Any]:
        """
        Process a single article through the specified steps with intelligent configuration.

        Args:
            article_id: ID of the article to process
            steps: List of processing steps to execute
            auto_publish: Whether to automatically publish after processing
            priority: Processing priority (low, normal, high)

        Returns:
            Processing result with task information
        """
        if steps is None:
            # æ ¹æ®æ–‡ç« ç±»å‹é€‰æ‹©ä¸åŒçš„å¤„ç†æµç¨‹
            # éœ€è¦å…ˆè·å–æ–‡ç« ä¿¡æ¯æ¥åˆ¤æ–­ç±»å‹
            async with get_db_session() as session:
                article = self._get_article(session, article_id)
                if article and hasattr(article, 'creation_type') and article.creation_type == 'topic_creation':
                    # ä¸»é¢˜åˆ›ä½œæµç¨‹ï¼šåˆ›ä½œï¼ˆåŒ…å«AIæ£€æµ‹å¾ªç¯ï¼‰
                    steps = [ProcessingStep.CREATE]
                    self.logger.info("ğŸ“ æ£€æµ‹åˆ°ä¸»é¢˜åˆ›ä½œæ–‡ç« ï¼Œä½¿ç”¨CREATEæµç¨‹")
                else:
                    # URLå¯¼å…¥æµç¨‹ï¼šæå– -> ç¿»è¯‘ -> ä¼˜åŒ–ï¼ˆåŒ…å«AIæ£€æµ‹å¾ªç¯ï¼‰
                    steps = [ProcessingStep.EXTRACT, ProcessingStep.TRANSLATE, ProcessingStep.OPTIMIZE]
                    self.logger.info("ğŸ”— æ£€æµ‹åˆ°URLå¯¼å…¥æ–‡ç« ï¼Œä½¿ç”¨EXTRACT->TRANSLATE->OPTIMIZEæµç¨‹")

        if auto_publish and ProcessingStep.PUBLISH not in steps:
            steps.append(ProcessingStep.PUBLISH)

        # Get intelligent processing configuration
        processing_config = await self._get_processing_configuration(article_id)
        self.logger.info(f"ğŸ¯ æ™ºèƒ½é…ç½®: {processing_config}")
        
        self.logger.info("="*80)
        self.logger.info(f"ğŸš€ å¼€å§‹æ–‡ç« å¤„ç†æµç¨‹ - æ–‡ç«  ID: {article_id}")
        self.logger.info(f"ğŸ“‹ å¤„ç†æ­¥éª¤: {steps}")
        self.logger.info(f"ğŸ”¢ ä¼˜å…ˆçº§: {priority}")
        self.logger.info(f"ğŸ“¤ è‡ªåŠ¨å‘å¸ƒ: {auto_publish}")
        self.logger.info("="*80)

        self.logger.info(f"Starting article processing for article {article_id} with steps: {steps}")

        try:
            # Get article from database
            self.logger.info("ğŸ“¦ æ­£åœ¨è·å–æ•°æ®åº“ä¼šè¯...")
            async with get_db_session() as session:
                self.logger.info("âœ… æ•°æ®åº“ä¼šè¯è·å–æˆåŠŸ")
                self.logger.info("ğŸ” æ­£åœ¨è·å–æ–‡ç« æ•°æ®...")
                article = self._get_article(session, article_id)
                self.logger.info(f"ğŸ“‹ æ–‡ç« è·å–ç»“æœ: {article}")
                if not article:
                    return {
                        "success": False,
                        "error": f"Article {article_id} not found"
                    }
                
                # Create processing task
                task_id = f"process_{article_id}_{int(datetime.utcnow().timestamp())}"
                self.logger.info(f"ğŸ”§ æ­£åœ¨åˆ›å»ºå¤„ç†ä»»åŠ¡: {task_id}")
                task = self._create_processing_task(session, task_id, article_id, steps)
                self.logger.info(f"âœ… å¤„ç†ä»»åŠ¡åˆ›å»ºå®Œæˆ: {task.task_id}")
                
                # Start processing in background with proper exception handling
                self.logger.info("ğŸš€ å‡†å¤‡å¯åŠ¨åå°å¤„ç†ä»»åŠ¡...")
                self.logger.info(f"ğŸ“‹ ä»»åŠ¡åç¨‹: {self._execute_processing_pipeline}")
                self.logger.info(f"ğŸ“‹ æ–‡ç« å¯¹è±¡: {article}")
                self.logger.info(f"ğŸ“‹ å¤„ç†æ­¥éª¤: {steps}")
                self.logger.info(f"ğŸ“‹ ä»»åŠ¡å¯¹è±¡: {task}")

                task_coroutine = self._execute_processing_pipeline(article, steps, task)
                self.logger.info(f"ğŸ“‹ åç¨‹å¯¹è±¡åˆ›å»ºæˆåŠŸ: {task_coroutine}")

                self.logger.info("ğŸ”¥ æ­£åœ¨åˆ›å»ºå¼‚æ­¥ä»»åŠ¡...")
                background_task = asyncio.create_task(task_coroutine)
                self.logger.info(f"âœ… å¼‚æ­¥ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {background_task}")

                # Add exception handler for background task
                def handle_task_exception(task):
                    self.logger.info(f"ğŸ” å¼‚æ­¥ä»»åŠ¡å®Œæˆå›è°ƒè¢«è°ƒç”¨: {task}")
                    try:
                        result = task.result()
                        self.logger.info(f"âœ… å¼‚æ­¥ä»»åŠ¡æ­£å¸¸å®Œæˆ: {result}")
                    except Exception as e:
                        self.logger.error(f"ğŸ’¥ åå°å¤„ç†ä»»åŠ¡å¼‚å¸¸ - æ–‡ç«  ID: {article.id}")
                        self.logger.error(f"ğŸ’¥ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
                        import traceback
                        self.logger.error(f"ğŸ’¥ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")

                self.logger.info("ğŸ”§ æ·»åŠ å¼‚æ­¥ä»»åŠ¡å®Œæˆå›è°ƒ...")
                background_task.add_done_callback(handle_task_exception)
                self.logger.info("âœ… å¼‚æ­¥ä»»åŠ¡å¯åŠ¨å®Œæˆï¼")
                
                return {
                    "success": True,
                    "task_id": task_id,
                    "article_id": article_id,
                    "status": "processing",
                    "steps": steps,
                    "priority": priority
                }
                
        except Exception as e:
            self.logger.error(f"Failed to start processing for article {article_id}: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def batch_process_articles(
        self,
        article_ids: List[int],
        operation: str = "process",
        parameters: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Process multiple articles in batch.
        
        Args:
            article_ids: List of article IDs to process
            operation: Type of operation (process, translate, optimize, etc.)
            parameters: Additional parameters for processing
            
        Returns:
            Batch processing result
        """
        if parameters is None:
            parameters = {}
        
        self.logger.info(f"Starting batch processing for {len(article_ids)} articles")
        
        try:
            batch_task_id = f"batch_{int(datetime.utcnow().timestamp())}"
            
            # Process each article
            results = []
            for article_id in article_ids:
                result = await self.process_article(
                    article_id,
                    steps=parameters.get('steps'),
                    auto_publish=parameters.get('auto_publish', False),
                    priority=parameters.get('priority', 'normal')
                )
                results.append({
                    "article_id": article_id,
                    "result": result
                })
            
            return {
                "success": True,
                "batch_task_id": batch_task_id,
                "article_count": len(article_ids),
                "operation": operation,
                "results": results
            }
            
        except Exception as e:
            self.logger.error(f"Failed to batch process articles: {e}")
            return {
                "success": False,
                "error": str(e)
            }

    async def _get_processing_configuration(self, article_id: int) -> Dict[str, Any]:
        """Get intelligent processing configuration for an article."""
        try:
            from .processing_config_service import get_processing_config_service

            async with get_db_session() as session:
                article = session.query(Article).filter(Article.id == article_id).first()
                if not article:
                    self.logger.error(f"Article {article_id} not found")
                    return {}

                config_service = get_processing_config_service()
                config = config_service.get_processing_configuration(article)

                self.logger.info(f"ğŸ“‹ è·å–åˆ°æ™ºèƒ½é…ç½®: ç±»åˆ«={config.get('content_category')}, ç­–ç•¥={config.get('processing_strategy')}")
                return config

        except Exception as e:
            self.logger.error(f"Error getting processing configuration: {e}")
            return {}

    async def _execute_processing_pipeline(self, article: Article, steps: List[str], task: Task):
        """Execute the complete processing pipeline for an article."""
        pipeline_start_time = datetime.utcnow()

        self.logger.info("="*80)
        self.logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡ç«  ID: {article.id}")
        self.logger.info(f"ğŸ“ æ–‡ç« æ ‡é¢˜: {article.title}")
        self.logger.info(f"ğŸ”— æ¥æºURL: {article.source_url}")
        self.logger.info(f"ğŸ“± æ¥æºå¹³å°: {article.source_platform}")
        self.logger.info(f"âš™ï¸  å¤„ç†æ­¥éª¤: {' -> '.join(steps)}")
        self.logger.info(f"ğŸ• å¼€å§‹æ—¶é—´: {pipeline_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"ğŸ“‹ æ­¥éª¤è¯¦æƒ…: {steps}")
        self.logger.info(f"ğŸ”§ ä»»åŠ¡ID: {task.id}")
        self.logger.info("="*80)

        try:
            self.logger.info("ğŸ”„ å¼€å§‹æ‰§è¡Œå¤„ç†æµç¨‹...")
            async with get_db_session() as session:
                self.logger.info("âœ… æ•°æ®åº“ä¼šè¯å·²å»ºç«‹")

                # Update task status to running
                self.logger.info("ğŸ”„ æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­...")
                await self._update_task_status(session, task.id, TaskStatus.RUNNING)
                self.logger.info(f"âœ… ä»»åŠ¡çŠ¶æ€å·²æ›´æ–°ä¸ºè¿è¡Œä¸­ (Task ID: {task.task_id})")

                total_steps = len(steps)
                completed_steps = 0

                for step_index, step in enumerate(steps, 1):
                    step_start_time = datetime.utcnow()

                    self.logger.info("-"*60)
                    self.logger.info(f"ğŸ”„ æ­¥éª¤ {step_index}/{total_steps}: {step.upper()}")
                    self.logger.info(f"ğŸ• æ­¥éª¤å¼€å§‹æ—¶é—´: {step_start_time.strftime('%H:%M:%S')}")
                    self.logger.info("-"*60)

                    # Update article status
                    new_status = self._get_status_for_step(step)
                    await self._update_article_status(session, article.id, new_status)
                    self.logger.info(f"ğŸ“Š æ–‡ç« çŠ¶æ€å·²æ›´æ–°ä¸º: {new_status.value}")

                    # Execute the processing step
                    if step in self.processing_steps:
                        try:
                            if step == ProcessingStep.DETECT:
                                # Special handling for AI detection with loop
                                self.logger.info("ğŸ¤– å¼€å§‹AIæ£€æµ‹å¾ªç¯æµç¨‹...")
                                result = await self._execute_ai_detection_loop(article, session)
                            else:
                                self.logger.info(f"âš¡ æ‰§è¡Œå¤„ç†æ­¥éª¤: {step}")
                                self.logger.info(f"ğŸ“‹ æ­¥éª¤æ–¹æ³•: {self.processing_steps[step].__name__}")
                                self.logger.info(f"ğŸš€ å³å°†è°ƒç”¨æ­¥éª¤æ–¹æ³•...")

                                result = await self.processing_steps[step](article)

                                self.logger.info(f"ğŸ“Š æ­¥éª¤ {step} æ‰§è¡Œå®Œæˆ")
                                self.logger.info(f"ğŸ“Š ç»“æœç±»å‹: {type(result)}")
                                self.logger.info(f"ğŸ“Š æ‰§è¡ŒæˆåŠŸ: {result.success if hasattr(result, 'success') else 'Unknown'}")

                            step_end_time = datetime.utcnow()
                            step_duration = (step_end_time - step_start_time).total_seconds()

                            if result.success:
                                self.logger.info(f"âœ… æ­¥éª¤ '{step}' æ‰§è¡ŒæˆåŠŸ")
                                self.logger.info(f"â±ï¸  æ­¥éª¤è€—æ—¶: {step_duration:.2f}ç§’")
                                if result.data:
                                    for key, value in result.data.items():
                                        self.logger.info(f"ğŸ“ˆ {key}: {value}")
                                self.logger.info(f"ğŸ’¬ ç»“æœæ¶ˆæ¯: {result.message}")
                            else:
                                self.logger.error(f"âŒ æ­¥éª¤ '{step}' æ‰§è¡Œå¤±è´¥")
                                self.logger.error(f"â±ï¸  æ­¥éª¤è€—æ—¶: {step_duration:.2f}ç§’")
                                self.logger.error(f"ğŸ’¬ é”™è¯¯æ¶ˆæ¯: {result.message}")
                                await self._update_task_status(session, task.id, TaskStatus.FAILED, result.message)
                                await self._update_article_status(session, article.id, ArticleStatus.FAILED)

                                self.logger.error("="*80)
                                self.logger.error(f"ğŸ’¥ å¤„ç†æµç¨‹å¤±è´¥ - æ–‡ç«  ID: {article.id}")
                                self.logger.error(f"ğŸ’¥ å¤±è´¥æ­¥éª¤: {step}")
                                self.logger.error(f"ğŸ’¥ å¤±è´¥åŸå› : {result.message}")
                                self.logger.error("="*80)
                                return

                        except Exception as step_error:
                            step_end_time = datetime.utcnow()
                            step_duration = (step_end_time - step_start_time).total_seconds()

                            self.logger.error(f"ğŸ’¥ æ­¥éª¤ '{step}' æ‰§è¡Œå¼‚å¸¸")
                            self.logger.error(f"â±ï¸  æ­¥éª¤è€—æ—¶: {step_duration:.2f}ç§’")
                            self.logger.error(f"ğŸ’¬ å¼‚å¸¸ä¿¡æ¯: {str(step_error)}")
                            await self._update_task_status(session, task.id, TaskStatus.FAILED, str(step_error))
                            await self._update_article_status(session, article.id, ArticleStatus.FAILED)

                            self.logger.error("="*80)
                            self.logger.error(f"ğŸ’¥ å¤„ç†æµç¨‹å¼‚å¸¸ - æ–‡ç«  ID: {article.id}")
                            self.logger.error(f"ğŸ’¥ å¼‚å¸¸æ­¥éª¤: {step}")
                            self.logger.error(f"ğŸ’¥ å¼‚å¸¸è¯¦æƒ…: {str(step_error)}")
                            self.logger.error("="*80)
                            return

                    completed_steps += 1
                    progress = (completed_steps / total_steps) * 100
                    await self._update_task_progress(session, task.id, progress)

                    self.logger.info(f"ğŸ“Š è¿›åº¦æ›´æ–°: {progress:.1f}% ({completed_steps}/{total_steps})")
                    self.logger.info(f"âœ… æ­¥éª¤ '{step}' å®Œæˆ")

                # Mark task as completed
                await self._update_task_status(session, task.id, TaskStatus.COMPLETED)
                await self._update_article_status(session, article.id, ArticleStatus.OPTIMIZED)

                pipeline_end_time = datetime.utcnow()
                total_duration = (pipeline_end_time - pipeline_start_time).total_seconds()

                self.logger.info("="*80)
                self.logger.info(f"ğŸ‰ å¤„ç†æµç¨‹å®Œæˆ - æ–‡ç«  ID: {article.id}")
                self.logger.info(f"ğŸ• å®Œæˆæ—¶é—´: {pipeline_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
                self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {total_duration:.2f}ç§’")
                self.logger.info(f"ğŸ“Š å®Œæˆæ­¥éª¤: {completed_steps}/{total_steps}")
                self.logger.info(f"âœ… æœ€ç»ˆçŠ¶æ€: {ArticleStatus.OPTIMIZED.value}")
                self.logger.info("="*80)

        except Exception as e:
            pipeline_end_time = datetime.utcnow()
            total_duration = (pipeline_end_time - pipeline_start_time).total_seconds()

            self.logger.error("="*80)
            self.logger.error(f"ğŸ’¥ å¤„ç†æµç¨‹å‘ç”Ÿä¸¥é‡å¼‚å¸¸ - æ–‡ç«  ID: {article.id}")
            self.logger.error(f"ğŸ’¥ å¼‚å¸¸æ—¶é—´: {pipeline_end_time.strftime('%Y-%m-%d %H:%M:%S')}")
            self.logger.error(f"â±ï¸  è¿è¡Œæ—¶é•¿: {total_duration:.2f}ç§’")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            self.logger.error("="*80)

            async with get_db_session() as session:
                await self._update_task_status(session, task.id, TaskStatus.FAILED, str(e))
                await self._update_article_status(session, article.id, ArticleStatus.FAILED)
    
    async def _extract_content(self, article: Article) -> ProcessingResult:
        """Extract content from the article URL using Freedium.cfd."""
        try:
            self.logger.info("="*80)
            self.logger.info("ğŸ“¥ å¼€å§‹å†…å®¹æå–...")
            self.logger.info(f"ğŸ”— ç›®æ ‡URL: {article.source_url}")
            self.logger.info(f"ğŸ“± æ¥æºå¹³å°: {article.source_platform}")
            self.logger.info("="*80)

            self.logger.info("ğŸ“¦ æ­£åœ¨å¯¼å…¥å†…å®¹æå–å™¨...")
            from .content_extractor import get_content_extractor
            self.logger.info("âœ… å†…å®¹æå–å™¨æ¨¡å—å¯¼å…¥æˆåŠŸ")

            self.logger.info("ğŸ”§ æ­£åœ¨è·å–å†…å®¹æå–å™¨å®ä¾‹...")
            extractor = get_content_extractor()
            self.logger.info("âœ… å†…å®¹æå–å™¨å®ä¾‹è·å–æˆåŠŸ")

            self.logger.info("ğŸ”§ å†…å®¹æå–å™¨å·²åˆå§‹åŒ–")
            self.logger.info("ğŸŒ æ­£åœ¨é€šè¿‡ Freedium.cfd æå–å†…å®¹...")
            self.logger.info("ğŸ“‹ å³å°†è°ƒç”¨ extractor.extract_content æ–¹æ³•...")

            # Extract content using Freedium.cfd
            self.logger.info(f"ğŸš€ å¼€å§‹è°ƒç”¨æå–æ–¹æ³•ï¼ŒURL: {article.source_url}")
            result = await extractor.extract_content(article.source_url)
            self.logger.info(f"ğŸ“Š æå–æ–¹æ³•è°ƒç”¨å®Œæˆï¼Œç»“æœç±»å‹: {type(result)}")
            self.logger.info(f"ğŸ“Š æå–æˆåŠŸ: {result.success if hasattr(result, 'success') else 'Unknown'}")

            if result.success:
                self.logger.info("âœ… å†…å®¹æå–æˆåŠŸ!")

                # Update article with extracted content
                original_content_length = len(article.content_original) if article.content_original else 0
                article.content_original = result.content
                new_content_length = len(result.content)

                self.logger.info(f"ğŸ“ å†…å®¹é•¿åº¦: {new_content_length} å­—ç¬¦ (åŸ: {original_content_length})")

                if result.title and not article.title:
                    article.title = result.title
                    self.logger.info(f"ğŸ“° æ ‡é¢˜å·²æ›´æ–°: {result.title}")

                if hasattr(article, 'author') and result.author:
                    article.author = result.author
                    self.logger.info(f"ğŸ‘¤ ä½œè€…ä¿¡æ¯: {result.author}")

                if hasattr(article, 'word_count'):
                    article.word_count = result.word_count
                    self.logger.info(f"ğŸ”¢ å­—æ•°ç»Ÿè®¡: {result.word_count} è¯")

                if hasattr(article, 'estimated_reading_time'):
                    article.estimated_reading_time = result.reading_time
                    self.logger.info(f"â±ï¸  é¢„è®¡é˜…è¯»æ—¶é—´: {result.reading_time} åˆ†é’Ÿ")

                self.logger.info(f"ğŸ¯ æå–æ–¹æ³•: {result.extraction_method}")

                # æ˜¾ç¤ºæå–åˆ°çš„å…¨æ–‡å†…å®¹ï¼ˆæˆªå–å‰500å­—ç¬¦ï¼‰
                content_preview = result.content[:500] + "..." if len(result.content) > 500 else result.content
                self.logger.info("ğŸ“„ æå–åˆ°çš„å…¨æ–‡å†…å®¹é¢„è§ˆ:")
                self.logger.info("â”€" * 60)
                self.logger.info(content_preview)
                self.logger.info("â”€" * 60)

                self.logger.info("âœ… å†…å®¹æå–æ­¥éª¤å®Œæˆ")

                return ProcessingResult(True, "å†…å®¹æå–æˆåŠŸ", {
                    "extraction_method": result.extraction_method,
                    "word_count": result.word_count,
                    "reading_time": result.reading_time,
                    "content_length": new_content_length,
                    "title_updated": bool(result.title and not article.title)
                })
            else:
                self.logger.error("âŒ å†…å®¹æå–å¤±è´¥")
                self.logger.error(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {result.error}")
                return ProcessingResult(False, f"å†…å®¹æå–å¤±è´¥: {result.error}")

        except Exception as e:
            self.logger.error("ğŸ’¥ å†…å®¹æå–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"å†…å®¹æå–å¼‚å¸¸: {str(e)}")
    
    async def _translate_content(self, article: Article) -> ProcessingResult:
        """Translate article content using LLM API service with intelligent classification."""
        try:
            self.logger.info("ğŸŒ å¼€å§‹æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»...")

            # Check if content is available
            if not article.content_original:
                self.logger.error("âŒ æ²¡æœ‰å¯ç¿»è¯‘çš„åŸå§‹å†…å®¹")
                return ProcessingResult(False, "æ²¡æœ‰å¯ç¿»è¯‘çš„åŸå§‹å†…å®¹")

            # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»é¢˜åˆ›ä½œæ–‡ç«  - è·³è¿‡ç¿»è¯‘å’Œåˆ†ç±»
            is_topic_creation = (hasattr(article, 'creation_type') and
                               article.creation_type == 'topic_creation')

            if is_topic_creation:
                self.logger.info("ğŸ¨ æ£€æµ‹åˆ°ä¸»é¢˜åˆ›ä½œæ–‡ç« ï¼Œè·³è¿‡ç¿»è¯‘å’Œåˆ†ç±»æ­¥éª¤")
                self.logger.info("ğŸ“ ä¸»é¢˜åˆ›ä½œå†…å®¹å·²ç»æ˜¯ä¸­æ–‡ï¼Œæ— éœ€ç¿»è¯‘")
                self.logger.info("ğŸ·ï¸ ä¸»é¢˜åˆ›ä½œæ–‡ç« ç›´æ¥ä½¿ç”¨æç¤ºè¯ï¼Œæ— éœ€åˆ†ç±»åˆ¤æ–­")

                # å¯¹äºä¸»é¢˜åˆ›ä½œï¼Œç›´æ¥è®¾ç½®ç¿»è¯‘å†…å®¹ä¸ºåŸå§‹å†…å®¹
                article.content_translated = article.content_original

                # è®¾ç½®ä¸€ä¸ªé€šç”¨åˆ†ç±»ï¼Œé¿å…åˆ†ç±»é€»è¾‘å½±å“åç»­å¤„ç†
                article.category = "general"

                # ä¿å­˜åˆ°æ•°æ®åº“
                async with get_db_session() as session:
                    session.merge(article)
                    session.commit()
                    self.logger.info("ğŸ’¾ ä¸»é¢˜åˆ›ä½œæ–‡ç« ä¿¡æ¯å·²ä¿å­˜åˆ°æ•°æ®åº“")

                return ProcessingResult(True, "ä¸»é¢˜åˆ›ä½œæ–‡ç« è·³è¿‡ç¿»è¯‘å’Œåˆ†ç±»", {
                    "method": "topic_creation_skip",
                    "original_length": len(article.content_original),
                    "translated_length": len(article.content_original),
                    "length_change": 0,
                    "classification": {
                        "category": "general",
                        "confidence": 1.0,
                        "reasoning": "ä¸»é¢˜åˆ›ä½œæ–‡ç« ï¼Œè·³è¿‡åˆ†ç±»åˆ¤æ–­ï¼Œç›´æ¥ä½¿ç”¨æç¤ºè¯å¤„ç†",
                        "method": "topic_creation_skip"
                    }
                })

            original_length = len(article.content_original)
            self.logger.info(f"ğŸ“ åŸå§‹å†…å®¹é•¿åº¦: {original_length} å­—ç¬¦")

            # ä½¿ç”¨æ–°çš„æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»API
            from .real_ai_api_call import get_real_ai_api_call
            api_service = get_real_ai_api_call()
            self.logger.info("ğŸ”§ æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»æœåŠ¡å·²åˆå§‹åŒ–")

            self.logger.info(f"ğŸ“± æ¥æºå¹³å°: {article.source_platform}")
            self.logger.info(f"ğŸ”— æ–‡ç« URL: {article.source_url}")

            self.logger.info("ğŸš€ æ­£åœ¨è°ƒç”¨AIè¿›è¡Œæ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»...")

            # æ˜¾ç¤ºç¿»è¯‘ä½¿ç”¨çš„åŸå§‹å†…å®¹ï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
            content_preview = article.content_original[:300] + "..." if len(article.content_original) > 300 else article.content_original
            self.logger.info("ğŸ“„ å¾…ç¿»è¯‘çš„åŸå§‹å†…å®¹:")
            self.logger.info("â”€" * 60)
            self.logger.info(content_preview)
            self.logger.info("â”€" * 60)

            # è°ƒç”¨æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»API
            result = await api_service.translate_and_classify_article(
                title=article.title,
                content=article.content_original,
                source_url=article.source_url,
                target_language="ä¸­æ–‡"
            )

            if result["success"]:
                self.logger.info("âœ… æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»å®Œæˆ!")

                # æ›´æ–°æ–‡ç« çš„ç¿»è¯‘å†…å®¹
                article.content_translated = result["translated_content"]
                translated_length = len(result["translated_content"])

                self.logger.info(f"ğŸ“ ç¿»è¯‘åé•¿åº¦: {translated_length} å­—ç¬¦")
                self.logger.info(f"ğŸ“Š é•¿åº¦å˜åŒ–: {translated_length - original_length:+d} å­—ç¬¦")

                # æ›´æ–°æ–‡ç« åˆ†ç±»ä¿¡æ¯
                classification = result["classification"]
                article.category = classification["category"]

                self.logger.info("ğŸ·ï¸ æ™ºèƒ½åˆ†ç±»ç»“æœ:")
                self.logger.info(f"   ğŸ“‚ åˆ†ç±»: {classification['category']}")
                self.logger.info(f"   ğŸ¯ ç½®ä¿¡åº¦: {classification['confidence']:.2%}")
                self.logger.info(f"   ğŸ’­ ç†ç”±: {classification['reasoning']}")
                self.logger.info(f"   ğŸ”§ æ–¹æ³•: {classification['method']}")

                if result.get("usage"):
                    self.logger.info(f"ğŸ’° Tokenä½¿ç”¨æƒ…å†µ: {result['usage']}")

                # ä¿å­˜åˆ°æ•°æ®åº“
                async with get_db_session() as session:
                    session.merge(article)
                    session.commit()
                    self.logger.info("ğŸ’¾ ç¿»è¯‘å’Œåˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")

                # æ˜¾ç¤ºç¿»è¯‘ç»“æœï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
                translated_preview = result["translated_content"][:300] + "..." if len(result["translated_content"]) > 300 else result["translated_content"]
                self.logger.info("ğŸ“„ ç¿»è¯‘ç»“æœå†…å®¹:")
                self.logger.info("â”€" * 60)
                self.logger.info(translated_preview)
                self.logger.info("â”€" * 60)

                self.logger.info("âœ… æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»æ­¥éª¤å®Œæˆ")

                return ProcessingResult(True, "æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»æˆåŠŸ", {
                    "usage": result.get("usage", {}),
                    "original_length": original_length,
                    "translated_length": translated_length,
                    "length_change": translated_length - original_length,
                    "classification": classification,
                    "method": "ai_llm_classification"
                })
            else:
                self.logger.error("âŒ æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»å¤±è´¥")
                self.logger.error(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")

                # é™çº§å¤„ç†ï¼šä½¿ç”¨ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•
                self.logger.info("ğŸ”„ é™çº§ä½¿ç”¨ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•...")
                return await self._fallback_translate_content(article)

        except Exception as e:
            self.logger.error("ğŸ’¥ æ™ºèƒ½ç¿»è¯‘å’Œåˆ†ç±»è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")

            # é™çº§å¤„ç†ï¼šä½¿ç”¨ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•
            self.logger.info("ğŸ”„ é™çº§ä½¿ç”¨ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•...")
            return await self._fallback_translate_content(article)

    async def _fallback_translate_content(self, article: Article) -> ProcessingResult:
        """Fallback translation method using traditional LLM service."""
        try:
            self.logger.info("ğŸ”„ ä½¿ç”¨ä¼ ç»Ÿç¿»è¯‘æ–¹æ³•...")

            from .llm_api import get_llm_service
            llm_service = get_llm_service()
            self.logger.info("ğŸ”§ ä¼ ç»ŸLLMç¿»è¯‘æœåŠ¡å·²åˆå§‹åŒ–")

            # Determine source and target languages
            source_lang = "en" if article.source_platform == "medium" else "auto"
            target_lang = "zh"  # Default to Chinese

            # Translate content
            result = await llm_service.translate_content(
                content=article.content_original,
                title=article.title,
                source_language=source_lang,
                target_language=target_lang
            )

            if result.success:
                self.logger.info("âœ… ä¼ ç»Ÿç¿»è¯‘å®Œæˆ!")

                # Update article with translated content
                article.content_translated = result.content
                translated_length = len(result.content)

                # ä½¿ç”¨ä¼ ç»Ÿå…³é”®è¯åˆ†ç±»æ–¹æ³•
                from .processing_config_service import ProcessingConfigService
                config_service = ProcessingConfigService()
                category, confidence = config_service.classify_article(article)

                article.category = category.value

                self.logger.info("ğŸ·ï¸ ä¼ ç»Ÿåˆ†ç±»ç»“æœ:")
                self.logger.info(f"   ğŸ“‚ åˆ†ç±»: {category.value}")
                self.logger.info(f"   ğŸ¯ ç½®ä¿¡åº¦: {confidence:.2%}")
                self.logger.info(f"   ğŸ”§ æ–¹æ³•: keyword_based")

                # ä¿å­˜åˆ°æ•°æ®åº“
                async with get_db_session() as session:
                    session.merge(article)
                    session.commit()
                    self.logger.info("ğŸ’¾ ä¼ ç»Ÿç¿»è¯‘å’Œåˆ†ç±»ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")

                return ProcessingResult(True, "ä¼ ç»Ÿç¿»è¯‘å’Œåˆ†ç±»æˆåŠŸ", {
                    "model": getattr(result, 'model', 'unknown'),
                    "usage": getattr(result, 'usage', {}),
                    "classification": {
                        "category": category.value,
                        "confidence": confidence,
                        "method": "keyword_based"
                    },
                    "method": "fallback_translation"
                })
            else:
                self.logger.error("âŒ ä¼ ç»Ÿç¿»è¯‘å¤±è´¥")
                self.logger.error(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {result.error}")
                return ProcessingResult(False, f"ä¼ ç»Ÿç¿»è¯‘å¤±è´¥: {result.error}")

        except Exception as e:
            self.logger.error("ğŸ’¥ ä¼ ç»Ÿç¿»è¯‘è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"ä¼ ç»Ÿç¿»è¯‘å¼‚å¸¸: {str(e)}")

    async def _optimize_content(self, article: Article) -> ProcessingResult:
        """Optimize article content with AI detection loop until AI concentration < 25%."""
        try:
            self.logger.info("âš¡ å¼€å§‹å†…å®¹ä¼˜åŒ–ä¸AIæ£€æµ‹å¾ªç¯...")

            from .llm_api import get_llm_service
            from .prompt_manager import get_prompt_manager, ContentType

            llm_service = get_llm_service()
            prompt_manager = get_prompt_manager()
            self.logger.info("ğŸ”§ LLMä¼˜åŒ–æœåŠ¡å’Œæç¤ºè¯ç®¡ç†å™¨å·²åˆå§‹åŒ–")

            # Use translated content if available, otherwise original
            content_to_optimize = article.content_translated or article.content_original
            content_source = "ç¿»è¯‘åå†…å®¹" if article.content_translated else "åŸå§‹å†…å®¹"

            self.logger.info(f"ğŸ“ ä¼˜åŒ–å†…å®¹æ¥æº: {content_source}")

            if not content_to_optimize:
                self.logger.error("âŒ æ²¡æœ‰å¯ä¼˜åŒ–çš„å†…å®¹")
                return ProcessingResult(False, "æ²¡æœ‰å¯ä¼˜åŒ–çš„å†…å®¹")

            original_length = len(content_to_optimize)
            original_word_count = len(content_to_optimize.split())

            self.logger.info(f"ğŸ“ å¾…ä¼˜åŒ–å†…å®¹é•¿åº¦: {original_length} å­—ç¬¦")
            self.logger.info(f"ğŸ”¢ å¾…ä¼˜åŒ–è¯æ•°: {original_word_count} è¯")

            # ç¡®å®šå†…å®¹ç±»å‹
            content_type = self._determine_content_type(article.title, content_to_optimize)
            self.logger.info(f"ğŸ“‹ å†…å®¹ç±»å‹: {content_type.value}")

            # Optimize content for the target platform (default: toutiao)
            target_platform = "toutiao"  # Default platform
            optimization_type = "standard"

            self.logger.info(f"ğŸ¯ ç›®æ ‡å¹³å°: {target_platform}")
            self.logger.info(f"âš™ï¸  ä¼˜åŒ–ç±»å‹: {optimization_type}")

            # æ˜¾ç¤ºå¾…ä¼˜åŒ–çš„å†…å®¹ï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
            content_preview = content_to_optimize[:300] + "..." if len(content_to_optimize) > 300 else content_to_optimize
            self.logger.info("ğŸ“„ å¾…ä¼˜åŒ–çš„å†…å®¹:")
            self.logger.info("â”€" * 60)
            self.logger.info(content_preview)
            self.logger.info("â”€" * 60)

            # å¼€å§‹ä¼˜åŒ–ä¸æ£€æµ‹å¾ªç¯
            from ..core.config import get_ai_optimization_config
            ai_config = get_ai_optimization_config()
            max_attempts = ai_config.max_attempts  # ä»é…ç½®è·å–æœ€å¤§ä¼˜åŒ–å°è¯•æ¬¡æ•°
            ai_threshold = ai_config.threshold  # ä»é…ç½®è·å–AIæµ“åº¦é˜ˆå€¼

            self.logger.info("ğŸ”„ å¼€å§‹ä¼˜åŒ–ä¸AIæ£€æµ‹å¾ªç¯...")
            self.logger.info(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {ai_threshold}%")
            self.logger.info(f"ğŸ”¢ æœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")

            current_content = content_to_optimize

            for attempt in range(1, max_attempts + 1):
                attempt_start_time = datetime.utcnow()

                self.logger.info("â•" * 60)
                self.logger.info(f"ğŸ”„ ç¬¬ {attempt}/{max_attempts} æ¬¡ä¼˜åŒ–å°è¯•")
                self.logger.info(f"ğŸ• å°è¯•å¼€å§‹æ—¶é—´: {attempt_start_time.strftime('%H:%M:%S')}")
                self.logger.info("â•" * 60)

                # ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
                optimization_prompt = prompt_manager.get_optimization_prompt(
                    content=current_content,
                    ai_probability=50.0 if attempt == 1 else 80.0,  # åç»­å°è¯•å‡è®¾æ›´é«˜AIæ¦‚ç‡
                    round_number=attempt,
                    content_type=content_type,
                    platform=target_platform
                )

                # æ˜¾ç¤ºä½¿ç”¨çš„ä¼˜åŒ–promptï¼ˆæˆªå–å‰400å­—ç¬¦ï¼‰
                prompt_preview = optimization_prompt[:400] + "..." if len(optimization_prompt) > 400 else optimization_prompt
                self.logger.info("ğŸ“ ä½¿ç”¨çš„ä¼˜åŒ–Prompt:")
                self.logger.info("â”€" * 60)
                self.logger.info(prompt_preview)
                self.logger.info("â”€" * 60)

                self.logger.info("ğŸš€ æ­£åœ¨è°ƒç”¨LLM APIè¿›è¡Œå†…å®¹ä¼˜åŒ–...")

                # æ‰§è¡Œä¼˜åŒ–
                result = await llm_service.optimize_content(
                    content=current_content,
                    title=article.title,
                    platform=target_platform,
                    optimization_type=optimization_type,
                    custom_prompt=optimization_prompt
                )

                if not result.success:
                    self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡ä¼˜åŒ–å¤±è´¥: {result.error}")
                    if attempt == max_attempts:
                        return ProcessingResult(False, f"å†…å®¹ä¼˜åŒ–åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥: {result.error}")
                    self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                    continue

                self.logger.info("âœ… å†…å®¹ä¼˜åŒ–å®Œæˆ!")

                # æ›´æ–°å½“å‰å†…å®¹ä¸ºä¼˜åŒ–åçš„å†…å®¹
                current_content = result.content
                optimized_length = len(current_content)
                optimized_word_count = len(current_content.split())

                self.logger.info(f"ğŸ“ ä¼˜åŒ–åé•¿åº¦: {optimized_length} å­—ç¬¦")
                self.logger.info(f"ğŸ”¢ ä¼˜åŒ–åè¯æ•°: {optimized_word_count} è¯")
                self.logger.info(f"ğŸ“Š é•¿åº¦å˜åŒ–: {optimized_length - original_length:+d} å­—ç¬¦")
                self.logger.info(f"ğŸ“Š è¯æ•°å˜åŒ–: {optimized_word_count - original_word_count:+d} è¯")

                # æ˜¾ç¤ºä¼˜åŒ–ç»“æœï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
                optimized_preview = current_content[:300] + "..." if len(current_content) > 300 else current_content
                self.logger.info("ğŸ“„ ä¼˜åŒ–ç»“æœå†…å®¹:")
                self.logger.info("â”€" * 60)
                self.logger.info(optimized_preview)
                self.logger.info("â”€" * 60)

                # ç«‹å³è¿›è¡ŒAIæ£€æµ‹
                self.logger.info("ğŸ¤– å¼€å§‹AIæ£€æµ‹...")

                # ä¸´æ—¶æ›´æ–°æ–‡ç« å†…å®¹ä»¥ä¾¿æ£€æµ‹
                original_optimized_content = article.content_optimized
                article.content_optimized = current_content

                detection_result = await self._detect_content(article)

                if not detection_result.success:
                    self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡AIæ£€æµ‹å¤±è´¥: {detection_result.message}")
                    # æ¢å¤åŸå§‹å†…å®¹
                    article.content_optimized = original_optimized_content
                    if attempt == max_attempts:
                        return ProcessingResult(False, f"AIæ£€æµ‹åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")
                    self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                    continue

                ai_probability = detection_result.data.get('ai_probability', 100.0)
                attempt_end_time = datetime.utcnow()
                attempt_duration = (attempt_end_time - attempt_start_time).total_seconds()

                self.logger.info(f"ğŸ“Š æ£€æµ‹ç»“æœ: {ai_probability}% AIæ¦‚ç‡")
                self.logger.info(f"â±ï¸  æœ¬æ¬¡å°è¯•è€—æ—¶: {attempt_duration:.2f}ç§’")

                # æ£€æŸ¥AIæµ“åº¦æ˜¯å¦ä½äºé˜ˆå€¼
                if ai_probability < ai_threshold:
                    self.logger.info("ğŸ‰ AIæ£€æµ‹é€šè¿‡!")
                    self.logger.info(f"âœ… AIæ¦‚ç‡ ({ai_probability}%) ä½äºé˜ˆå€¼ ({ai_threshold}%)")
                    self.logger.info(f"ğŸ“Š ä½¿ç”¨å°è¯•æ¬¡æ•°: {attempt}/{max_attempts}")

                    # æœ€ç»ˆæ›´æ–°æ–‡ç« å†…å®¹å’Œå…ƒæ•°æ®
                    article.content_optimized = current_content

                    if hasattr(article, 'word_count'):
                        article.word_count = optimized_word_count
                        self.logger.info(f"ğŸ”¢ æ–‡ç« è¯æ•°å·²æ›´æ–°: {optimized_word_count}")

                    if hasattr(article, 'estimated_reading_time'):
                        reading_time = max(1, optimized_word_count // 200)
                        article.estimated_reading_time = reading_time
                        self.logger.info(f"â±ï¸  é¢„è®¡é˜…è¯»æ—¶é—´å·²æ›´æ–°: {reading_time} åˆ†é’Ÿ")

                    self.logger.info("âœ… å†…å®¹ä¼˜åŒ–ä¸AIæ£€æµ‹å¾ªç¯å®Œæˆ")

                    return ProcessingResult(True, f"å†…å®¹ä¼˜åŒ–æˆåŠŸï¼ŒAIæ£€æµ‹é€šè¿‡: {ai_probability}% AIæ¦‚ç‡", {
                        "model": getattr(result, 'model', 'unknown'),
                        "usage": getattr(result, 'usage', {}),
                        "finish_reason": getattr(result, 'finish_reason', 'unknown'),
                        "original_length": original_length,
                        "optimized_length": optimized_length,
                        "original_word_count": original_word_count,
                        "optimized_word_count": optimized_word_count,
                        "length_change": optimized_length - original_length,
                        "word_count_change": optimized_word_count - original_word_count,
                        "platform": target_platform,
                        "optimization_type": optimization_type,
                        "ai_probability": ai_probability,
                        "attempts_used": attempt,
                        "threshold": ai_threshold
                    })
                else:
                    # AIæµ“åº¦è¿‡é«˜ï¼Œéœ€è¦é‡æ–°ä¼˜åŒ–
                    self.logger.warning(f"âš ï¸  AIæ¦‚ç‡ ({ai_probability}%) è¶…è¿‡é˜ˆå€¼ ({ai_threshold}%)")

                    if attempt < max_attempts:
                        self.logger.info("ğŸ”„ éœ€è¦é‡æ–°ä¼˜åŒ–å†…å®¹ä»¥é™ä½AIç—•è¿¹...")
                        # ç»§ç»­ä¸‹ä¸€æ¬¡å¾ªç¯ï¼Œä½¿ç”¨å½“å‰ä¼˜åŒ–çš„å†…å®¹ä½œä¸ºä¸‹æ¬¡ä¼˜åŒ–çš„è¾“å…¥
                    else:
                        # è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
                        self.logger.error("ğŸ’¥ ä¼˜åŒ–ä¸AIæ£€æµ‹å¾ªç¯å¤±è´¥!")
                        self.logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts})")
                        self.logger.error(f"ğŸ“Š æœ€ç»ˆAIæ¦‚ç‡: {ai_probability}%")
                        self.logger.error(f"ğŸ¯ è¦æ±‚é˜ˆå€¼: {ai_threshold}%")

                        return ProcessingResult(False, f"å†…å®¹ä¼˜åŒ–å¤±è´¥: AIæ¦‚ç‡ {ai_probability}% ä»è¶…è¿‡é˜ˆå€¼ {ai_threshold}%", {
                            "ai_probability": ai_probability,
                            "attempts_used": max_attempts,
                            "threshold": ai_threshold,
                            "final_status": "failed_ai_detection"
                        })

            # å¦‚æœåˆ°è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
            return ProcessingResult(False, f"å†…å®¹ä¼˜åŒ–åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")

        except Exception as e:
            self.logger.error("ğŸ’¥ å†…å®¹ä¼˜åŒ–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"ä¼˜åŒ–å¼‚å¸¸: {str(e)}")

    async def _create_content(self, article: Article) -> ProcessingResult:
        """Create content based on topic for topic-based articles."""
        try:
            self.logger.info("="*80)
            self.logger.info("ğŸ¨ å¼€å§‹ä¸»é¢˜å†…å®¹åˆ›ä½œ...")

            # Check if this is a topic-based creation
            if not hasattr(article, 'creation_type') or article.creation_type != 'topic_creation':
                self.logger.warning("âš ï¸  éä¸»é¢˜åˆ›ä½œæ–‡ç« ï¼Œè·³è¿‡åˆ›ä½œæ­¥éª¤")
                return ProcessingResult(True, "éä¸»é¢˜åˆ›ä½œæ–‡ç« ï¼Œè·³è¿‡åˆ›ä½œæ­¥éª¤")

            if not hasattr(article, 'topic') or not article.topic:
                self.logger.error("âŒ ç¼ºå°‘åˆ›ä½œä¸»é¢˜")
                return ProcessingResult(False, "ç¼ºå°‘åˆ›ä½œä¸»é¢˜")

            self.logger.info(f"ğŸ¯ åˆ›ä½œä¸»é¢˜: {article.topic}")

            # Get keywords if available
            keywords = []
            if hasattr(article, 'keywords') and article.keywords:
                try:
                    import json
                    keywords = json.loads(article.keywords) if isinstance(article.keywords, str) else article.keywords
                    self.logger.info(f"ğŸ·ï¸  å…³é”®è¯: {', '.join(keywords)}")
                except:
                    self.logger.warning("âš ï¸  å…³é”®è¯è§£æå¤±è´¥")

            # Get creation requirements
            requirements = ""
            if hasattr(article, 'creation_requirements') and article.creation_requirements:
                requirements = article.creation_requirements
                self.logger.info(f"ğŸ“‹ åˆ›ä½œè¦æ±‚: {requirements}")

            # Import LLM service
            from .llm_api import get_llm_service
            llm_service = get_llm_service()
            self.logger.info("ğŸ”§ LLMåˆ›ä½œæœåŠ¡å·²åˆå§‹åŒ–")

            # Get creation prompt from database if specified
            creation_prompt = None
            if hasattr(article, 'selected_creation_prompt_id') and article.selected_creation_prompt_id:
                creation_prompt = self._get_creation_prompt_template(article.selected_creation_prompt_id, article.topic, keywords, requirements)
                self.logger.info(f"ğŸ“ ä½¿ç”¨æ•°æ®åº“æç¤ºè¯æ¨¡æ¿ ID: {article.selected_creation_prompt_id}")

            if not creation_prompt:
                creation_prompt = self._build_creation_prompt(article.topic, keywords, requirements)
                self.logger.info("ğŸ“ ä½¿ç”¨é»˜è®¤åˆ›ä½œæç¤ºè¯")

            # Display the creation prompt (first 300 characters)
            prompt_preview = creation_prompt[:300] + "..." if len(creation_prompt) > 300 else creation_prompt
            self.logger.info("ğŸ“„ åˆ›ä½œæç¤ºè¯é¢„è§ˆ:")
            self.logger.info("â”€" * 60)
            self.logger.info(prompt_preview)
            self.logger.info("â”€" * 60)

            self.logger.info("ğŸš€ æ­£åœ¨è°ƒç”¨LLM APIè¿›è¡Œå†…å®¹åˆ›ä½œ...")

            # è·å–ç›®æ ‡é•¿åº¦è®¾ç½®
            target_length = getattr(article, 'target_length', 'mini')
            self.logger.info(f"ğŸ“ æ–‡ç« ç›®æ ‡é•¿åº¦: {target_length}")

            # è®¾ç½®å½“å‰ç›®æ ‡é•¿åº¦ï¼Œä¾›æ¨¡æ¿å¤„ç†ä½¿ç”¨
            self._current_target_length = target_length

            # è·å–APIå‚æ•°é…ç½®ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            api_params = {}
            if hasattr(article, 'selected_model_id') and article.selected_model_id:
                # ä»æ•°æ®åº“è·å–æ¨¡å‹é…ç½®
                try:
                    from ..core.database import get_db_connection
                    conn = get_db_connection()
                    cursor = conn.cursor()
                    cursor.execute("""
                        SELECT temperature, max_tokens, top_p, frequency_penalty, presence_penalty
                        FROM api_models WHERE id = ?
                    """, (article.selected_model_id,))
                    model_config = cursor.fetchone()
                    conn.close()

                    if model_config:
                        api_params = {
                            'temperature': model_config[0] or 0.7,
                            'max_tokens': model_config[1] or 4000,
                            'top_p': model_config[2] or 1.0,
                            'frequency_penalty': model_config[3] or 0.0,
                            'presence_penalty': model_config[4] or 0.0
                        }
                        self.logger.info(f"ğŸ”§ ä½¿ç”¨æ¨¡å‹é…ç½®: {api_params}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸  è·å–æ¨¡å‹é…ç½®å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°: {e}")

            # Create content using LLM
            result = await llm_service.create_content_by_topic(
                topic=article.topic,
                keywords=keywords,
                requirements=requirements,
                custom_prompt=creation_prompt,
                target_length=target_length,
                **api_params
            )

            if result.success:
                self.logger.info("âœ… ä¸»é¢˜å†…å®¹åˆ›ä½œå®Œæˆ!")

                # Update article with created content
                article.content_original = result.content
                created_length = len(result.content)
                created_word_count = len(result.content.split())

                # Calculate reading time early to avoid scope issues
                reading_time = max(1, created_word_count // 200)

                self.logger.info(f"ğŸ“ åˆ›ä½œå†…å®¹é•¿åº¦: {created_length} å­—ç¬¦")
                self.logger.info(f"ğŸ”¢ åˆ›ä½œè¯æ•°: {created_word_count} è¯")

                # Update metadata
                if hasattr(article, 'word_count'):
                    article.word_count = created_word_count
                    self.logger.info(f"ğŸ”¢ æ–‡ç« è¯æ•°å·²æ›´æ–°: {created_word_count}")

                if hasattr(article, 'estimated_reading_time'):
                    article.estimated_reading_time = reading_time
                    self.logger.info(f"â±ï¸  é¢„è®¡é˜…è¯»æ—¶é—´: {reading_time} åˆ†é’Ÿ")

                # Update title if not set or is default
                if not article.title or article.title.startswith("ä¸»é¢˜åˆ›ä½œ:"):
                    if hasattr(result, 'title') and result.title:
                        article.title = result.title
                        self.logger.info(f"ğŸ“° æ–‡ç« æ ‡é¢˜å·²æ›´æ–°: {result.title}")

                # Display created content (first 500 characters)
                content_preview = result.content[:500] + "..." if len(result.content) > 500 else result.content
                self.logger.info("ğŸ“„ åˆ›ä½œå†…å®¹é¢„è§ˆ:")
                self.logger.info("â”€" * 60)
                self.logger.info(content_preview)
                self.logger.info("â”€" * 60)

                # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šåˆ›ä½œå®Œæˆåç«‹å³è¿›è¡ŒAIæ£€æµ‹
                self.logger.info("ğŸ¤– å¼€å§‹å¯¹åˆ›ä½œå†…å®¹è¿›è¡ŒAIæ£€æµ‹...")

                # æ‰§è¡ŒAIæ£€æµ‹ä¸ä¼˜åŒ–å¾ªç¯
                detection_and_optimization_result = await self._create_content_detection_loop(article)

                if detection_and_optimization_result.success:
                    self.logger.info("âœ… ä¸»é¢˜å†…å®¹åˆ›ä½œä¸AIæ£€æµ‹å®Œæˆ")

                    # åˆå¹¶ç»“æœæ•°æ®
                    final_data = {
                        "topic": article.topic,
                        "keywords": keywords,
                        "content_length": len(article.content_original),
                        "word_count": len(article.content_original.split()) if article.content_original else 0,
                        "reading_time": reading_time,
                        "title_updated": bool(hasattr(result, 'title') and result.title),
                        "ai_detection": detection_and_optimization_result.data if hasattr(detection_and_optimization_result, 'data') else {}
                    }

                    return ProcessingResult(True, f"ä¸»é¢˜å†…å®¹åˆ›ä½œæˆåŠŸï¼Œ{detection_and_optimization_result.message}", final_data)
                else:
                    self.logger.error("âŒ AIæ£€æµ‹ä¸ä¼˜åŒ–å¤±è´¥")
                    return ProcessingResult(False, f"ä¸»é¢˜å†…å®¹åˆ›ä½œæˆåŠŸï¼Œä½†AIæ£€æµ‹å¤±è´¥: {detection_and_optimization_result.message}")

            else:
                self.logger.error("âŒ ä¸»é¢˜å†…å®¹åˆ›ä½œå¤±è´¥")
                self.logger.error(f"ğŸ’¬ å¤±è´¥åŸå› : {result.message}")
                return ProcessingResult(False, f"ä¸»é¢˜å†…å®¹åˆ›ä½œå¤±è´¥: {result.message}")

        except Exception as e:
            self.logger.error("ğŸ’¥ ä¸»é¢˜å†…å®¹åˆ›ä½œè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"ä¸»é¢˜å†…å®¹åˆ›ä½œå¼‚å¸¸: {str(e)}")

    async def _create_content_detection_loop(self, article: Article) -> ProcessingResult:
        """
        å¯¹åˆ›ä½œçš„å†…å®¹è¿›è¡ŒAIæ£€æµ‹ï¼Œå¦‚æœä¸é€šè¿‡åˆ™å¯åŠ¨ä¼˜åŒ–å¾ªç¯ã€‚
        è¿™æ˜¯é’ˆå¯¹ä¸»é¢˜åˆ›ä½œå†…å®¹çš„æ£€æµ‹ä¸ä¼˜åŒ–æµç¨‹ã€‚
        """
        try:
            from ..core.config import get_ai_optimization_config
            ai_config = get_ai_optimization_config()
            max_attempts = ai_config.max_attempts  # ä»é…ç½®è·å–æœ€å¤§ä¼˜åŒ–å°è¯•æ¬¡æ•°
            ai_threshold = ai_config.threshold  # ä»é…ç½®è·å–AIæµ“åº¦é˜ˆå€¼

            self.logger.info("ğŸ”„ å¼€å§‹åˆ›ä½œå†…å®¹çš„AIæ£€æµ‹ä¸ä¼˜åŒ–å¾ªç¯...")
            self.logger.info(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {ai_threshold}%")
            self.logger.info(f"ğŸ”¢ æœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")

            # è·å–å½“å‰åˆ›ä½œçš„å†…å®¹
            current_content = article.content_original
            if not current_content:
                return ProcessingResult(False, "æ²¡æœ‰åˆ›ä½œå†…å®¹å¯ä¾›æ£€æµ‹")

            for attempt in range(1, max_attempts + 1):
                attempt_start_time = datetime.utcnow()

                self.logger.info("â•" * 60)
                self.logger.info(f"ğŸ”„ ç¬¬ {attempt}/{max_attempts} æ¬¡æ£€æµ‹å°è¯•")
                self.logger.info(f"ğŸ• å°è¯•å¼€å§‹æ—¶é—´: {attempt_start_time.strftime('%H:%M:%S')}")
                self.logger.info("â•" * 60)

                # æ‰§è¡ŒAIæ£€æµ‹
                self.logger.info("ğŸ¤– æ‰§è¡ŒAIæ£€æµ‹...")

                # ä¸´æ—¶è®¾ç½®å†…å®¹ä»¥ä¾¿æ£€æµ‹
                original_content = article.content_original
                article.content_original = current_content

                detection_result = await self._detect_content(article)

                # æ¢å¤åŸå§‹å†…å®¹
                article.content_original = original_content

                if not detection_result.success:
                    self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡AIæ£€æµ‹å¤±è´¥: {detection_result.message}")
                    if attempt == max_attempts:
                        return ProcessingResult(False, f"AIæ£€æµ‹åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")
                    self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                    continue

                ai_probability = detection_result.data.get('ai_probability', 100.0)
                attempt_end_time = datetime.utcnow()
                attempt_duration = (attempt_end_time - attempt_start_time).total_seconds()

                self.logger.info(f"ğŸ“Š æ£€æµ‹ç»“æœ: {ai_probability}% AIæ¦‚ç‡")
                self.logger.info(f"â±ï¸  æœ¬æ¬¡å°è¯•è€—æ—¶: {attempt_duration:.2f}ç§’")

                # æ£€æŸ¥AIæµ“åº¦æ˜¯å¦ä½äºé˜ˆå€¼
                if ai_probability < ai_threshold:
                    self.logger.info("ğŸ‰ AIæ£€æµ‹é€šè¿‡!")
                    self.logger.info(f"âœ… AIæ¦‚ç‡ ({ai_probability}%) ä½äºé˜ˆå€¼ ({ai_threshold}%)")
                    self.logger.info(f"ğŸ“Š ä½¿ç”¨å°è¯•æ¬¡æ•°: {attempt}/{max_attempts}")

                    # æ›´æ–°æ–‡ç« å†…å®¹ä¸ºæœ€ç»ˆç‰ˆæœ¬
                    article.content_original = current_content

                    return ProcessingResult(True, f"AIæ£€æµ‹é€šè¿‡: {ai_probability}% AIæ¦‚ç‡", {
                        "ai_probability": ai_probability,
                        "attempts_used": attempt,
                        "threshold": ai_threshold,
                        "final_status": "passed",
                        "content_length": len(current_content),
                        "optimization_applied": attempt > 1
                    })
                else:
                    # AIæµ“åº¦è¿‡é«˜ï¼Œéœ€è¦ä¼˜åŒ–
                    self.logger.warning(f"âš ï¸  AIæ¦‚ç‡ ({ai_probability}%) è¶…è¿‡é˜ˆå€¼ ({ai_threshold}%)")

                    if attempt < max_attempts:
                        self.logger.info("ğŸ”„ éœ€è¦ä¼˜åŒ–å†…å®¹ä»¥é™ä½AIç—•è¿¹...")

                        # æ‰§è¡Œå†…å®¹ä¼˜åŒ–
                        optimization_result = await self._optimize_for_ai_detection(current_content, ai_probability, attempt)

                        if optimization_result.success:
                            current_content = optimization_result.content
                            self.logger.info(f"âœ… ç¬¬ {attempt} æ¬¡ä¼˜åŒ–å®Œæˆ")

                            # æ˜¾ç¤ºä¼˜åŒ–åçš„å†…å®¹é¢„è§ˆ
                            optimized_preview = current_content[:300] + "..." if len(current_content) > 300 else current_content
                            self.logger.info("ğŸ“„ ä¼˜åŒ–åå†…å®¹é¢„è§ˆ:")
                            self.logger.info("â”€" * 60)
                            self.logger.info(optimized_preview)
                            self.logger.info("â”€" * 60)
                        else:
                            self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡ä¼˜åŒ–å¤±è´¥: {optimization_result.message}")
                            if attempt == max_attempts:
                                return ProcessingResult(False, f"å†…å®¹ä¼˜åŒ–åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")
                            self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                            continue
                    else:
                        # è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°
                        self.logger.error("ğŸ’¥ åˆ›ä½œå†…å®¹AIæ£€æµ‹ä¸ä¼˜åŒ–å¾ªç¯å¤±è´¥!")
                        self.logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts})")
                        self.logger.error(f"ğŸ“Š æœ€ç»ˆAIæ¦‚ç‡: {ai_probability}%")
                        self.logger.error(f"ğŸ¯ è¦æ±‚é˜ˆå€¼: {ai_threshold}%")

                        return ProcessingResult(False, f"åˆ›ä½œå†…å®¹AIæ£€æµ‹å¤±è´¥: AIæ¦‚ç‡ {ai_probability}% ä»è¶…è¿‡é˜ˆå€¼ {ai_threshold}%", {
                            "ai_probability": ai_probability,
                            "attempts_used": max_attempts,
                            "threshold": ai_threshold,
                            "final_status": "failed_ai_detection"
                        })

            # å¦‚æœåˆ°è¿™é‡Œï¼Œè¯´æ˜æ‰€æœ‰å°è¯•éƒ½å¤±è´¥äº†
            return ProcessingResult(False, f"åˆ›ä½œå†…å®¹AIæ£€æµ‹åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")

        except Exception as e:
            self.logger.error("ğŸ’¥ åˆ›ä½œå†…å®¹AIæ£€æµ‹å¾ªç¯è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"åˆ›ä½œå†…å®¹AIæ£€æµ‹å¾ªç¯å¼‚å¸¸: {str(e)}")

    def _build_creation_prompt(self, topic: str, keywords: list, requirements: str) -> str:
        """Build creation prompt for topic-based content creation."""
        prompt_parts = [
            "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„æ–‡ç« ï¼š",
            "",
            f"ä¸»é¢˜ï¼š{topic}",
        ]

        if keywords:
            prompt_parts.extend([
                f"å…³é”®è¯ï¼š{', '.join(keywords)}",
            ])

        if requirements:
            prompt_parts.extend([
                f"åˆ›ä½œè¦æ±‚ï¼š{requirements}",
            ])

        prompt_parts.extend([
            "",
            "è¯·ç¡®ä¿æ–‡ç« ï¼š",
            "1. å†…å®¹åŸåˆ›ä¸”æœ‰æ·±åº¦",
            "2. ç»“æ„æ¸…æ™°ï¼Œé€»è¾‘æ€§å¼º",
            "3. è¯­è¨€æµç•…ï¼Œç¬¦åˆä¸­æ–‡è¡¨è¾¾ä¹ æƒ¯",
            "4. åŒ…å«å®ç”¨ä»·å€¼å’Œè§è§£",
            "5. å­—æ•°åœ¨1000-3000å­—ä¹‹é—´",
            "",
            "è¯·ç›´æ¥è¾“å‡ºæ–‡ç« å†…å®¹ï¼Œä¸éœ€è¦é¢å¤–çš„è¯´æ˜ã€‚"
        ])

        return "\n".join(prompt_parts)

    def _get_creation_prompt_template(self, prompt_id: int, topic: str, keywords: list, requirements: str) -> str:
        """Get creation prompt template from database and fill variables."""
        try:
            from ..core.database import get_db_connection
            conn = get_db_connection()
            cursor = conn.cursor()

            cursor.execute("SELECT template, name, display_name FROM prompt_templates WHERE id = ? AND is_active = 1", (prompt_id,))
            row = cursor.fetchone()
            conn.close()

            if row:
                template = row[0]
                template_name = row[1]
                template_display_name = row[2]

                self.logger.info(f"ğŸ“ ä½¿ç”¨æç¤ºè¯æ¨¡æ¿: {template_display_name} ({template_name})")

                # æ˜¾ç¤ºåŸå§‹æ¨¡æ¿å†…å®¹ï¼ˆå‰200å­—ç¬¦ï¼‰
                template_preview = template[:200] + "..." if len(template) > 200 else template
                self.logger.info("ğŸ“„ åŸå§‹æ¨¡æ¿å†…å®¹é¢„è§ˆ:")
                self.logger.info("â”€" * 60)
                self.logger.info(template_preview)
                self.logger.info("â”€" * 60)

                # Replace template variables
                keywords_str = ', '.join(keywords) if keywords else ''

                # å®‰å…¨çš„å˜é‡æ›¿æ¢ï¼Œé¿å…æ ¼å¼åŒ–é”™è¯¯
                try:
                    # Fill template variables - æä¾›æ‰€æœ‰å¯èƒ½çš„å˜é‡
                    filled_template = template.format(
                        topic=topic,
                        title=topic,  # æ·»åŠ titleå˜é‡ï¼Œä½¿ç”¨topicä½œä¸ºå€¼
                        keywords=keywords_str,
                        requirements=requirements or 'è¯·åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„æ–‡ç« ã€‚'
                    )

                    self.logger.info("âœ… æç¤ºè¯æ¨¡æ¿å˜é‡å¡«å……å®Œæˆ")

                    # æ£€æŸ¥æ¨¡æ¿æ˜¯å¦åŒ…å«è§’è‰²å®šä½ä¿¡æ¯ï¼Œå¦‚æœæ˜¯ï¼Œéœ€è¦æ·»åŠ æ˜ç¡®çš„åˆ›ä½œæŒ‡ä»¤
                    if "è§’è‰²å®šä½" in filled_template or "æƒ…æ„Ÿæ•…äº‹ä½œå®¶" in filled_template:
                        self.logger.info("ğŸ­ æ£€æµ‹åˆ°è§’è‰²å®šä½æ¨¡æ¿ï¼Œæ·»åŠ æ˜ç¡®çš„åˆ›ä½œæŒ‡ä»¤")

                        # åœ¨æ¨¡æ¿æœ«å°¾æ·»åŠ æ˜ç¡®çš„åˆ›ä½œæŒ‡ä»¤ï¼ŒåŒ…å«ç›®æ ‡é•¿åº¦
                        # è·å–ç›®æ ‡é•¿åº¦æ˜ å°„
                        length_mapping = {
                            "mini": "300-500",
                            "short": "500-800",
                            "medium": "800-1500",
                            "long": "1500-3000"
                        }

                        # ä»å½“å‰è®¾ç½®çš„ç›®æ ‡é•¿åº¦è·å–å­—æ•°è¦æ±‚
                        target_length = getattr(self, '_current_target_length', 'mini')
                        word_count = length_mapping.get(target_length, "300-500")

                        self.logger.info(f"ğŸ¯ è§’è‰²æ¨¡æ¿ä¸­ä½¿ç”¨çš„ç›®æ ‡é•¿åº¦: {target_length}")
                        self.logger.info(f"ğŸ“ å¯¹åº”çš„å­—æ•°è¦æ±‚: {word_count}")

                        creation_instruction = f"""

**ğŸ“ æœ¬æ¬¡åˆ›ä½œä»»åŠ¡ï¼š**
è¯·æ ¹æ®ä»¥ä¸Šè§’è‰²å®šä½å’Œå†™ä½œè¦æ±‚ï¼Œå›´ç»•ä¸»é¢˜"{topic}"åˆ›ä½œä¸€ç¯‡æ–‡ç« ã€‚

ä¸»é¢˜ï¼š{topic}
å…³é”®è¯ï¼š{keywords_str}
åˆ›ä½œè¦æ±‚ï¼š{requirements or 'è¯·åˆ›ä½œä¸€ç¯‡é«˜è´¨é‡çš„æ–‡ç« ã€‚'}
å­—æ•°è¦æ±‚ï¼š{word_count} å­—

è¯·ç›´æ¥å¼€å§‹åˆ›ä½œæ–‡ç« å†…å®¹ï¼Œä¸è¦å†é‡å¤è§’è‰²å®šä½è¯´æ˜ã€‚æ–‡ç« åº”è¯¥ï¼š
1. ç´§æ‰£ä¸»é¢˜"{topic}"
2. ä½“ç°ä¸Šè¿°å†™ä½œé£æ ¼å’Œç»“æ„ç‰¹ç‚¹
3. å†…å®¹åŸåˆ›ä¸”æœ‰æ·±åº¦
4. å­—æ•°ä¸¥æ ¼æ§åˆ¶åœ¨ {word_count} å­—ä¹‹é—´

ç°åœ¨è¯·å¼€å§‹åˆ›ä½œï¼š"""

                        filled_template += creation_instruction
                        self.logger.info("âœ… å·²æ·»åŠ æ˜ç¡®çš„åˆ›ä½œæŒ‡ä»¤")

                    # æ˜¾ç¤ºå¡«å……åçš„æ¨¡æ¿å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰
                    filled_preview = filled_template[:500] + "..." if len(filled_template) > 500 else filled_template
                    self.logger.info("ğŸ“„ å¡«å……åæ¨¡æ¿å†…å®¹é¢„è§ˆ:")
                    self.logger.info("â”€" * 60)
                    self.logger.info(filled_preview)
                    self.logger.info("â”€" * 60)

                    return filled_template

                except KeyError as ke:
                    self.logger.error(f"âŒ æ¨¡æ¿å˜é‡å¡«å……å¤±è´¥ï¼Œç¼ºå°‘å˜é‡: {ke}")
                    self.logger.error("ğŸ’¡ æ¨¡æ¿å¯èƒ½åŒ…å«æœªå®šä¹‰çš„å˜é‡ï¼Œä½¿ç”¨åŸå§‹æ¨¡æ¿")
                    return template
                except Exception as fe:
                    self.logger.error(f"âŒ æ¨¡æ¿æ ¼å¼åŒ–å¤±è´¥: {fe}")
                    self.logger.error("ğŸ’¡ ä½¿ç”¨åŸå§‹æ¨¡æ¿")
                    return template

            else:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æç¤ºè¯æ¨¡æ¿ ID: {prompt_id}")
                return None

        except Exception as e:
            self.logger.error(f"ğŸ’¥ è·å–æç¤ºè¯æ¨¡æ¿å¤±è´¥: {str(e)}")
            return None

    def _determine_content_type(self, title: str, content: str) -> 'ContentType':
        """Determine content type based on title and content."""
        from .prompt_manager import ContentType

        title_lower = title.lower() if title else ""
        content_lower = content.lower() if content else ""

        # Technical keywords
        tech_keywords = [
            'ai', 'machine learning', 'deep learning', 'neural network', 'algorithm',
            'programming', 'python', 'javascript', 'react', 'vue', 'angular',
            'api', 'database', 'sql', 'nosql', 'cloud', 'aws', 'azure',
            'docker', 'kubernetes', 'microservices', 'devops', 'git',
            'blockchain', 'cryptocurrency', 'web3', 'smart contract',
            'äººå·¥æ™ºèƒ½', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'ç®—æ³•',
            'ç¼–ç¨‹', 'ç¨‹åº', 'ä»£ç ', 'å¼€å‘', 'æŠ€æœ¯', 'è½¯ä»¶', 'ç¡¬ä»¶',
            'æ•°æ®åº“', 'äº‘è®¡ç®—', 'åŒºå—é“¾', 'åŠ å¯†è´§å¸'
        ]

        # Tutorial keywords
        tutorial_keywords = [
            'how to', 'tutorial', 'guide', 'step by step', 'learn',
            'beginner', 'introduction', 'getting started', 'basics',
            'æ•™ç¨‹', 'æŒ‡å—', 'å…¥é—¨', 'å­¦ä¹ ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'æ­¥éª¤'
        ]

        # News keywords
        news_keywords = [
            'news', 'breaking', 'report', 'announcement', 'release',
            'update', 'latest', 'today', 'yesterday', 'this week',
            'æ–°é—»', 'æŠ¥é“', 'å‘å¸ƒ', 'æ›´æ–°', 'æœ€æ–°', 'ä»Šæ—¥', 'æ˜¨æ—¥'
        ]

        # Check for technical content
        tech_count = sum(1 for keyword in tech_keywords
                        if keyword in title_lower or keyword in content_lower[:500])

        # Check for tutorial content
        tutorial_count = sum(1 for keyword in tutorial_keywords
                           if keyword in title_lower or keyword in content_lower[:500])

        # Check for news content
        news_count = sum(1 for keyword in news_keywords
                        if keyword in title_lower or keyword in content_lower[:500])

        # Determine content type based on keyword counts
        if tech_count >= 2:
            return ContentType.TECHNICAL
        elif tutorial_count >= 1:
            return ContentType.TUTORIAL
        elif news_count >= 1:
            return ContentType.NEWS
        else:
            return ContentType.GENERAL

    async def _detect_content(self, article: Article) -> ProcessingResult:
        """Detect AI-generated content using Zhuque detection service (single detection only)."""
        try:
            self.logger.info("ğŸ¤– å¼€å§‹AIå†…å®¹æ£€æµ‹ï¼ˆç¡®è®¤æ£€æµ‹ï¼‰...")

            from .ai_detection import get_ai_detector
            detector = get_ai_detector()
            self.logger.info("ğŸ”§ AIæ£€æµ‹æœåŠ¡å·²åˆå§‹åŒ–")

            # Get the content to detect (use optimized content if available, otherwise translated or original)
            content_to_detect = (
                article.content_optimized or
                article.content_translated or
                article.content_original
            )

            # Determine content source
            if article.content_optimized:
                content_source = "ä¼˜åŒ–åå†…å®¹"
            elif article.content_translated:
                content_source = "ç¿»è¯‘åå†…å®¹"
            else:
                content_source = "åŸå§‹å†…å®¹"

            self.logger.info(f"ğŸ“ æ£€æµ‹å†…å®¹æ¥æº: {content_source}")

            # ä¸¥æ ¼éªŒè¯å†…å®¹ä¸ä¸ºç©º
            if not content_to_detect or len(content_to_detect.strip()) == 0:
                self.logger.error("âŒ æ²¡æœ‰å¯æ£€æµ‹çš„å†…å®¹")
                self.logger.error(f"âŒ æ–‡ç« ID: {article.id}")
                self.logger.error(f"âŒ æ–‡ç« æ ‡é¢˜: {article.title}")
                self.logger.error(f"âŒ åŸå§‹å†…å®¹: {'ç©º' if not article.content_original else f'{len(article.content_original)}å­—ç¬¦'}")
                self.logger.error(f"âŒ ç¿»è¯‘å†…å®¹: {'ç©º' if not article.content_translated else f'{len(article.content_translated)}å­—ç¬¦'}")
                self.logger.error(f"âŒ ä¼˜åŒ–å†…å®¹: {'ç©º' if not article.content_optimized else f'{len(article.content_optimized)}å­—ç¬¦'}")
                return ProcessingResult(False, "æ²¡æœ‰å¯æ£€æµ‹çš„å†…å®¹")

            content_length = len(content_to_detect)
            self.logger.info(f"ğŸ“ å¾…æ£€æµ‹å†…å®¹é•¿åº¦: {content_length} å­—ç¬¦")

            # æ˜¾ç¤ºå¾…æ£€æµ‹çš„å†…å®¹ï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
            content_preview = content_to_detect[:300] + "..." if len(content_to_detect) > 300 else content_to_detect
            self.logger.info("ğŸ“„ å¾…æ£€æµ‹çš„å†…å®¹:")
            self.logger.info("â”€" * 60)
            self.logger.info(content_preview)
            self.logger.info("â”€" * 60)

            # æ‰§è¡Œå•æ¬¡AIæ£€æµ‹ï¼ˆä¸å¾ªç¯ï¼Œå› ä¸ºä¼˜åŒ–æ­¥éª¤å·²ç»å¤„ç†äº†å¾ªç¯ï¼‰
            self.logger.info("ğŸš€ æ‰§è¡ŒAIæ£€æµ‹...")
            detection_result = await detector.detect_ai_content(content_to_detect)

            if detection_result.success:
                ai_probability = detection_result.ai_probability
                ai_threshold = 25.0

                self.logger.info(f"ğŸ“Š AIæ£€æµ‹ç»“æœ: {ai_probability}% AIæ¦‚ç‡")
                self.logger.info(f"ğŸ¯ é˜ˆå€¼æ ‡å‡†: {ai_threshold}%")

                if ai_probability < ai_threshold:
                    self.logger.info("âœ… AIæ£€æµ‹é€šè¿‡!")
                    self.logger.info(f"âœ… AIæ¦‚ç‡ ({ai_probability}%) ä½äºé˜ˆå€¼ ({ai_threshold}%)")
                    status_message = f"AIæ£€æµ‹é€šè¿‡: {ai_probability}% AIæ¦‚ç‡"
                else:
                    self.logger.warning("âš ï¸ AIæ£€æµ‹æœªé€šè¿‡!")
                    self.logger.warning(f"âš ï¸ AIæ¦‚ç‡ ({ai_probability}%) è¶…è¿‡é˜ˆå€¼ ({ai_threshold}%)")
                    self.logger.warning("ğŸ’¡ æ³¨æ„: å¦‚æœè¿™æ˜¯åœ¨OPTIMIZEæ­¥éª¤ä¹‹åï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥ä¼˜åŒ–é€»è¾‘")
                    status_message = f"AIæ£€æµ‹æœªé€šè¿‡: {ai_probability}% AIæ¦‚ç‡è¶…è¿‡é˜ˆå€¼"

                return ProcessingResult(True, status_message, {
                    "ai_probability": ai_probability,
                    "threshold": ai_threshold,
                    "passed": ai_probability < ai_threshold,
                    "content_length": content_length,
                    "content_source": content_source
                })
            else:
                self.logger.error("âŒ AIæ£€æµ‹å¤±è´¥")
                error_msg = detection_result.error or "æœªçŸ¥é”™è¯¯"
                self.logger.error(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {error_msg}")
                return ProcessingResult(False, f"AIæ£€æµ‹å¤±è´¥: {error_msg}")

        except Exception as e:
            self.logger.error("ğŸ’¥ AIæ£€æµ‹è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"AIæ£€æµ‹å¼‚å¸¸: {str(e)}")

    async def _execute_ai_detection_loop(self, article: Article, session) -> ProcessingResult:
        """
        Execute AI detection with optimization loop until AI concentration < 25%.

        Args:
            article: Article to process
            session: Database session

        Returns:
            ProcessingResult indicating success or failure
        """
        from ..core.config import get_ai_optimization_config
        ai_config = get_ai_optimization_config()
        max_attempts = ai_config.max_attempts  # ä»é…ç½®è·å–æœ€å¤§ä¼˜åŒ–å°è¯•æ¬¡æ•°
        ai_threshold = ai_config.threshold  # ä»é…ç½®è·å–AIæµ“åº¦é˜ˆå€¼

        loop_start_time = datetime.utcnow()

        self.logger.info("ğŸ”„ å¼€å§‹AIæ£€æµ‹ä¼˜åŒ–å¾ªç¯...")
        self.logger.info(f"ğŸ¯ ç›®æ ‡é˜ˆå€¼: {ai_threshold}%")
        self.logger.info(f"ğŸ”¢ æœ€å¤§å°è¯•æ¬¡æ•°: {max_attempts}")
        self.logger.info(f"ğŸ• å¾ªç¯å¼€å§‹æ—¶é—´: {loop_start_time.strftime('%H:%M:%S')}")

        for attempt in range(1, max_attempts + 1):
            attempt_start_time = datetime.utcnow()

            self.logger.info("â”€" * 50)
            self.logger.info(f"ğŸ”„ ç¬¬ {attempt}/{max_attempts} æ¬¡å°è¯•")
            self.logger.info(f"ğŸ• å°è¯•å¼€å§‹æ—¶é—´: {attempt_start_time.strftime('%H:%M:%S')}")

            # Perform AI detection
            self.logger.info("ğŸ¤– æ‰§è¡ŒAIæ£€æµ‹...")
            detection_result = await self._detect_content(article)

            if not detection_result.success:
                self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡AIæ£€æµ‹å¤±è´¥: {detection_result.message}")
                if attempt == max_attempts:
                    self.logger.error(f"ğŸ’¥ AIæ£€æµ‹åœ¨ {max_attempts} æ¬¡å°è¯•åä»ç„¶å¤±è´¥")
                    return ProcessingResult(False, f"AIæ£€æµ‹åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")
                self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                continue

            ai_probability = detection_result.data.get('ai_probability', 100.0)
            attempt_end_time = datetime.utcnow()
            attempt_duration = (attempt_end_time - attempt_start_time).total_seconds()

            self.logger.info(f"ğŸ“Š æ£€æµ‹ç»“æœ: {ai_probability}% AIæ¦‚ç‡")
            self.logger.info(f"â±ï¸  æœ¬æ¬¡å°è¯•è€—æ—¶: {attempt_duration:.2f}ç§’")

            # Check if AI concentration is below threshold
            if ai_probability < ai_threshold:
                loop_end_time = datetime.utcnow()
                total_duration = (loop_end_time - loop_start_time).total_seconds()

                self.logger.info("ğŸ‰ AIæ£€æµ‹å¾ªç¯æˆåŠŸå®Œæˆ!")
                self.logger.info(f"âœ… AIæ¦‚ç‡ ({ai_probability}%) ä½äºé˜ˆå€¼ ({ai_threshold}%)")
                self.logger.info(f"ğŸ“Š ä½¿ç”¨å°è¯•æ¬¡æ•°: {attempt}/{max_attempts}")
                self.logger.info(f"â±ï¸  æ€»å¾ªç¯è€—æ—¶: {total_duration:.2f}ç§’")
                self.logger.info("ğŸš€ æ–‡ç« å·²å‡†å¤‡å¥½å‘å¸ƒ!")

                return ProcessingResult(True, f"AIæ£€æµ‹é€šè¿‡: {ai_probability}% AIæ¦‚ç‡", {
                    "ai_probability": ai_probability,
                    "attempts_used": attempt,
                    "threshold": ai_threshold,
                    "total_duration": total_duration,
                    "final_status": "ready_for_publish"
                })

            # AI concentration too high, need to re-optimize
            self.logger.warning(f"âš ï¸  AIæ¦‚ç‡ ({ai_probability}%) è¶…è¿‡é˜ˆå€¼ ({ai_threshold}%)")

            if attempt < max_attempts:
                self.logger.info("ğŸ”„ éœ€è¦é‡æ–°ä¼˜åŒ–å†…å®¹ä»¥é™ä½AIç—•è¿¹...")

                # Re-optimize content to reduce AI traces
                reopt_start_time = datetime.utcnow()
                optimization_result = await self._re_optimize_for_ai_reduction(article)
                reopt_end_time = datetime.utcnow()
                reopt_duration = (reopt_end_time - reopt_start_time).total_seconds()

                if not optimization_result.success:
                    self.logger.error(f"âŒ ç¬¬ {attempt} æ¬¡é‡æ–°ä¼˜åŒ–å¤±è´¥: {optimization_result.message}")
                    self.logger.error(f"â±ï¸  é‡æ–°ä¼˜åŒ–è€—æ—¶: {reopt_duration:.2f}ç§’")
                    if attempt == max_attempts:
                        return ProcessingResult(False, f"å†…å®¹é‡æ–°ä¼˜åŒ–åœ¨ {max_attempts} æ¬¡å°è¯•åå¤±è´¥")
                    self.logger.info("ğŸ”„ ç»§ç»­ä¸‹ä¸€æ¬¡å°è¯•...")
                    continue

                self.logger.info(f"âœ… ç¬¬ {attempt} æ¬¡é‡æ–°ä¼˜åŒ–æˆåŠŸ")
                self.logger.info(f"â±ï¸  é‡æ–°ä¼˜åŒ–è€—æ—¶: {reopt_duration:.2f}ç§’")
                self.logger.info("ğŸ”„ å‡†å¤‡è¿›è¡Œä¸‹ä¸€æ¬¡AIæ£€æµ‹...")
            else:
                # Maximum attempts reached
                loop_end_time = datetime.utcnow()
                total_duration = (loop_end_time - loop_start_time).total_seconds()

                self.logger.error("ğŸ’¥ AIæ£€æµ‹å¾ªç¯å¤±è´¥!")
                self.logger.error(f"âŒ å·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•° ({max_attempts})")
                self.logger.error(f"ğŸ“Š æœ€ç»ˆAIæ¦‚ç‡: {ai_probability}%")
                self.logger.error(f"ğŸ¯ è¦æ±‚é˜ˆå€¼: {ai_threshold}%")
                self.logger.error(f"â±ï¸  æ€»å¾ªç¯è€—æ—¶: {total_duration:.2f}ç§’")

                return ProcessingResult(False, f"æ— æ³•å°†AIæ¦‚ç‡é™ä½åˆ° {ai_threshold}% ä»¥ä¸‹ï¼Œç»è¿‡ {max_attempts} æ¬¡å°è¯•åæœ€ç»ˆæ¦‚ç‡ä¸º {ai_probability}%")

        # This should not be reached, but just in case
        self.logger.error("ğŸ’¥ AIæ£€æµ‹å¾ªç¯æ„å¤–ç»“æŸ")
        return ProcessingResult(False, "AIæ£€æµ‹å¾ªç¯æ„å¤–å®Œæˆ")

    async def _re_optimize_for_ai_reduction(self, article: Article) -> ProcessingResult:
        """
        Re-optimize content specifically to reduce AI detection traces.

        Args:
            article: Article to re-optimize

        Returns:
            ProcessingResult indicating success or failure
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹AIç—•è¿¹é™ä½ä¼˜åŒ–...")

            from .llm_api import get_llm_service
            from .prompt_manager import get_prompt_manager

            llm_service = get_llm_service()
            prompt_manager = get_prompt_manager()
            self.logger.info("ğŸ”§ LLMé‡æ–°ä¼˜åŒ–æœåŠ¡å’Œæç¤ºè¯ç®¡ç†å™¨å·²åˆå§‹åŒ–")

            # Get current content (use optimized if available)
            current_content = article.content_optimized or article.content_translated or article.content_original

            # Determine content source
            if article.content_optimized:
                content_source = "å½“å‰ä¼˜åŒ–å†…å®¹"
            elif article.content_translated:
                content_source = "ç¿»è¯‘å†…å®¹"
            else:
                content_source = "åŸå§‹å†…å®¹"

            self.logger.info(f"ğŸ“ é‡æ–°ä¼˜åŒ–å†…å®¹æ¥æº: {content_source}")

            if not current_content:
                self.logger.error("âŒ æ²¡æœ‰å¯é‡æ–°ä¼˜åŒ–çš„å†…å®¹")
                return ProcessingResult(False, "æ²¡æœ‰å¯é‡æ–°ä¼˜åŒ–çš„å†…å®¹")

            original_length = len(current_content)
            original_word_count = len(current_content.split())

            self.logger.info(f"ğŸ“ å½“å‰å†…å®¹é•¿åº¦: {original_length} å­—ç¬¦")
            self.logger.info(f"ğŸ”¢ å½“å‰è¯æ•°: {original_word_count} è¯")

            # æ˜¾ç¤ºå½“å‰å¾…é‡æ–°ä¼˜åŒ–çš„å†…å®¹ï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
            content_preview = current_content[:300] + "..." if len(current_content) > 300 else current_content
            self.logger.info("ğŸ“„ å¾…é‡æ–°ä¼˜åŒ–çš„å†…å®¹:")
            self.logger.info("â”€" * 60)
            self.logger.info(content_preview)
            self.logger.info("â”€" * 60)

            # ç¡®å®šå†…å®¹ç±»å‹
            content_type = self._determine_content_type(article.title, current_content)
            self.logger.info(f"ğŸ“‹ å†…å®¹ç±»å‹: {content_type.value}")

            # Create a specialized prompt for reducing AI traces using prompt manager
            self.logger.info("ğŸ“ æ„å»ºAIç—•è¿¹é™ä½ä¸“ç”¨æç¤ºè¯...")
            ai_reduction_prompt = prompt_manager.get_optimization_prompt(
                content=current_content,
                ai_probability=75.0,  # å‡è®¾é«˜AIæ¦‚ç‡ï¼Œéœ€è¦æ·±åº¦ä¼˜åŒ–
                round_number=2,  # è¿™æ˜¯é‡æ–°ä¼˜åŒ–ï¼Œç®—ä½œç¬¬äºŒè½®
                content_type=content_type,
                detection_feedback="éœ€è¦é™ä½AIç—•è¿¹",
                platform="toutiao"
            )

            # æ˜¾ç¤ºä½¿ç”¨çš„AIç—•è¿¹é™ä½prompt
            prompt_preview = ai_reduction_prompt[:400] + "..." if len(ai_reduction_prompt) > 400 else ai_reduction_prompt
            self.logger.info("ğŸ“ AIç—•è¿¹é™ä½ä¸“ç”¨Prompt:")
            self.logger.info("â”€" * 60)
            self.logger.info(prompt_preview)
            self.logger.info("â”€" * 60)

            self.logger.info("ğŸš€ æ­£åœ¨è°ƒç”¨LLM APIè¿›è¡ŒAIç—•è¿¹é™ä½ä¼˜åŒ–...")

            # Call LLM API with specialized prompt
            result = await llm_service._call_api(ai_reduction_prompt)

            if result.success and result.content:
                self.logger.info("âœ… AIç—•è¿¹é™ä½ä¼˜åŒ–å®Œæˆ!")

                # Update article with re-optimized content
                article.content_optimized = result.content
                new_length = len(result.content)
                new_word_count = len(result.content.split())

                self.logger.info(f"ğŸ“ é‡æ–°ä¼˜åŒ–åé•¿åº¦: {new_length} å­—ç¬¦")
                self.logger.info(f"ğŸ”¢ é‡æ–°ä¼˜åŒ–åè¯æ•°: {new_word_count} è¯")
                self.logger.info(f"ğŸ“Š é•¿åº¦å˜åŒ–: {new_length - original_length:+d} å­—ç¬¦")
                self.logger.info(f"ğŸ“Š è¯æ•°å˜åŒ–: {new_word_count - original_word_count:+d} è¯")

                if hasattr(result, 'model') and result.model:
                    self.logger.info(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹: {result.model}")

                if hasattr(result, 'usage') and result.usage:
                    self.logger.info(f"ğŸ’° Tokenä½¿ç”¨æƒ…å†µ: {result.usage}")

                # æ˜¾ç¤ºé‡æ–°ä¼˜åŒ–çš„ç»“æœï¼ˆæˆªå–å‰300å­—ç¬¦ï¼‰
                reoptimized_preview = result.content[:300] + "..." if len(result.content) > 300 else result.content
                self.logger.info("ğŸ“„ é‡æ–°ä¼˜åŒ–ç»“æœå†…å®¹:")
                self.logger.info("â”€" * 60)
                self.logger.info(reoptimized_preview)
                self.logger.info("â”€" * 60)

                self.logger.info("âœ… AIç—•è¿¹é™ä½ä¼˜åŒ–æ­¥éª¤å®Œæˆ")

                return ProcessingResult(True, "AIç—•è¿¹é™ä½ä¼˜åŒ–æˆåŠŸ", {
                    "model": getattr(result, 'model', 'unknown'),
                    "usage": getattr(result, 'usage', {}),
                    "original_length": original_length,
                    "new_length": new_length,
                    "original_word_count": original_word_count,
                    "new_word_count": new_word_count,
                    "length_change": new_length - original_length,
                    "word_count_change": new_word_count - original_word_count,
                    "optimization_type": "ai_reduction"
                })
            else:
                error_msg = getattr(result, 'error', 'æœªçŸ¥é”™è¯¯') if result else 'è°ƒç”¨å¤±è´¥'
                self.logger.error("âŒ AIç—•è¿¹é™ä½ä¼˜åŒ–å¤±è´¥")
                self.logger.error(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {error_msg}")
                return ProcessingResult(False, f"é‡æ–°ä¼˜åŒ–å¤±è´¥: {error_msg}")

        except Exception as e:
            self.logger.error("ğŸ’¥ AIç—•è¿¹é™ä½ä¼˜åŒ–è¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"é‡æ–°ä¼˜åŒ–å¼‚å¸¸: {str(e)}")
    
    async def _publish_content(self, article: Article) -> ProcessingResult:
        """Publish article to target platforms."""
        try:
            self.logger.info("ğŸ“¤ å¼€å§‹å†…å®¹å‘å¸ƒ...")

            # Check if content is ready for publishing
            final_content = article.content_optimized or article.content_translated or article.content_original

            if not final_content:
                self.logger.error("âŒ æ²¡æœ‰å¯å‘å¸ƒçš„å†…å®¹")
                return ProcessingResult(False, "æ²¡æœ‰å¯å‘å¸ƒçš„å†…å®¹")

            # Determine content source
            if article.content_optimized:
                content_source = "ä¼˜åŒ–åå†…å®¹"
            elif article.content_translated:
                content_source = "ç¿»è¯‘åå†…å®¹"
            else:
                content_source = "åŸå§‹å†…å®¹"

            self.logger.info(f"ğŸ“ å‘å¸ƒå†…å®¹æ¥æº: {content_source}")
            self.logger.info(f"ğŸ“ å‘å¸ƒå†…å®¹é•¿åº¦: {len(final_content)} å­—ç¬¦")
            self.logger.info(f"ğŸ“° æ–‡ç« æ ‡é¢˜: {article.title}")

            # Check AI detection status
            if hasattr(article, 'ai_detection_passed') and hasattr(article, 'ai_detection_score'):
                if article.ai_detection_passed:
                    self.logger.info(f"âœ… AIæ£€æµ‹å·²é€šè¿‡ (æ¦‚ç‡: {article.ai_detection_score}%)")
                else:
                    self.logger.warning(f"âš ï¸  AIæ£€æµ‹æœªé€šè¿‡ (æ¦‚ç‡: {article.ai_detection_score}%)ï¼Œä½†ä»ç»§ç»­å‘å¸ƒ")
            else:
                self.logger.info("â„¹ï¸  æœªè¿›è¡ŒAIæ£€æµ‹")

            # TODO: Implement actual content publishing to platforms
            self.logger.info("ğŸš€ æ­£åœ¨å‘å¸ƒåˆ°ç›®æ ‡å¹³å°...")
            self.logger.info("ğŸ“± ç›®æ ‡å¹³å°: ä»Šæ—¥å¤´æ¡ (é»˜è®¤)")

            # Simulate publishing process
            await asyncio.sleep(2)  # Simulate processing time

            self.logger.info("âœ… å†…å®¹å‘å¸ƒå®Œæˆ!")
            self.logger.info("ğŸ‰ æ–‡ç« å·²æˆåŠŸå‘å¸ƒåˆ°å¹³å°")

            return ProcessingResult(True, "å†…å®¹å‘å¸ƒæˆåŠŸ", {
                "content_source": content_source,
                "content_length": len(final_content),
                "title": article.title,
                "platform": "toutiao",
                "ai_detection_passed": getattr(article, 'ai_detection_passed', None),
                "ai_detection_score": getattr(article, 'ai_detection_score', None)
            })

        except Exception as e:
            self.logger.error("ğŸ’¥ å†…å®¹å‘å¸ƒè¿‡ç¨‹å‘ç”Ÿå¼‚å¸¸")
            self.logger.error(f"ğŸ’¬ å¼‚å¸¸è¯¦æƒ…: {str(e)}")
            return ProcessingResult(False, f"å‘å¸ƒå¼‚å¸¸: {str(e)}")
    
    def _get_status_for_step(self, step: str) -> ArticleStatus:
        """Get the appropriate article status for a processing step."""
        status_map = {
            ProcessingStep.CREATE: ArticleStatus.EXTRACTING,  # Use EXTRACTING for CREATE step
            ProcessingStep.EXTRACT: ArticleStatus.EXTRACTING,
            ProcessingStep.TRANSLATE: ArticleStatus.TRANSLATING,
            ProcessingStep.OPTIMIZE: ArticleStatus.OPTIMIZING,
            ProcessingStep.DETECT: ArticleStatus.DETECTING,
            ProcessingStep.PUBLISH: ArticleStatus.PUBLISHING
        }
        return status_map.get(step, ArticleStatus.PENDING)
    
    def _get_article(self, session, article_id: int) -> Optional[Article]:
        """Get article from database."""
        try:
            self.logger.info(f"ğŸ” æ­£åœ¨æŸ¥è¯¢æ–‡ç«  ID: {article_id}")
            cursor = session.execute("""
                SELECT id, title, source_url, source_platform, content_original,
                       content_translated, content_optimized, content_final, status,
                       creation_type, topic, keywords, selected_creation_prompt_id,
                       selected_model_id, creation_requirements
                FROM articles WHERE id = ?
            """, (article_id,))
            row = cursor.fetchone()
            self.logger.info(f"ğŸ“Š æ•°æ®åº“æŸ¥è¯¢ç»“æœ: {row}")

            if row:
                self.logger.info("âœ… æ‰¾åˆ°æ–‡ç« è®°å½•ï¼Œæ­£åœ¨åˆ›å»ºArticleå¯¹è±¡...")
                # Helper function to safely get values from sqlite3.Row
                def safe_get(row, key, default=None):
                    try:
                        return row[key] if row[key] is not None else default
                    except (KeyError, IndexError):
                        return default

                article = Article(
                    id=row['id'],
                    title=row['title'],
                    source_url=row['source_url'],
                    source_platform=row['source_platform'],
                    content_original=row['content_original'],
                    content_translated=row['content_translated'],
                    content_optimized=row['content_optimized'],
                    content_final=row['content_final'],
                    status=ArticleStatus(row['status']),
                    # Topic creation fields
                    creation_type=safe_get(row, 'creation_type', 'url_import'),
                    topic=safe_get(row, 'topic'),
                    keywords=safe_get(row, 'keywords'),
                    selected_creation_prompt_id=safe_get(row, 'selected_creation_prompt_id'),
                    selected_model_id=safe_get(row, 'selected_model_id'),
                    creation_requirements=safe_get(row, 'creation_requirements')
                )
                self.logger.info(f"âœ… Articleå¯¹è±¡åˆ›å»ºæˆåŠŸ: {article.title}")
                return article
            else:
                self.logger.warning(f"âš ï¸ æœªæ‰¾åˆ°æ–‡ç«  ID: {article_id}")
                self.logger.error("âŒ æ— æ³•æ‰¾åˆ°æ–‡ç« è®°å½•ï¼Œå¤„ç†ç»ˆæ­¢")
                return None
        except Exception as e:
            self.logger.error(f"âŒ è·å–æ–‡ç« å¤±è´¥ {article_id}: {e}")
            return None
    
    def _create_processing_task(self, session, task_id: str, article_id: int, steps: List[str]) -> Task:
        """Create a new processing task."""
        try:
            self.logger.info(f"ğŸ”§ æ­£åœ¨åˆ›å»ºå¤„ç†ä»»åŠ¡: {task_id}")
            steps_str = ",".join(steps)
            session.execute(
                """INSERT INTO tasks (task_id, name, type, status, article_id)
                   VALUES (?, ?, ?, ?, ?)""",
                (task_id, f"Process Article {article_id}", "article_processing", "pending", article_id)
            )
            session.commit()
            self.logger.info(f"âœ… å¤„ç†ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")

            task = Task(
                id=1,  # We'd need to get the actual ID from the database
                task_id=task_id,
                name=f"Process Article {article_id}",
                type="article_processing",
                article_id=article_id,
                status=TaskStatus.PENDING
            )
            self.logger.info(f"ğŸ“‹ Taskå¯¹è±¡åˆ›å»ºæˆåŠŸ: {task.task_id}")
            return task
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
            self.logger.info("ğŸ”„ è¿”å›æ¨¡æ‹Ÿä»»åŠ¡ç”¨äºæµ‹è¯•...")
            # Return mock task for testing
            return Task(
                id=1,
                task_id=task_id,
                name=f"Process Article {article_id}",
                type="article_processing",
                article_id=article_id,
                status=TaskStatus.PENDING
            )
    
    async def _update_task_status(self, session, task_id: int, status: TaskStatus, error_message: str = None):
        """Update task status in database."""
        try:
            session.execute(
                "UPDATE tasks SET status = ? WHERE task_id = ?",
                (status.value, str(task_id))
            )
            session.commit()
            self.logger.info(f"Task {task_id} status updated to {status}")
        except Exception as e:
            self.logger.error(f"Failed to update task status: {e}")

    async def _update_task_progress(self, session, task_id: int, progress: float):
        """Update task progress in database."""
        # For now, just log progress since we don't have a progress column
        self.logger.info(f"Task {task_id} progress updated to {progress}%")

    async def _update_article_status(self, session, article_id: int, status: ArticleStatus):
        """Update article status in database."""
        try:
            session.execute(
                "UPDATE articles SET status = ?, updated_at = CURRENT_TIMESTAMP WHERE id = ?",
                (status.value, article_id)
            )
            session.commit()
            self.logger.info(f"Article {article_id} status updated to {status}")
        except Exception as e:
            self.logger.error(f"Failed to update article status: {e}")

    async def _intelligent_detection_loop(self, article: Article, content: str, detector, max_rounds: int, threshold: float):
        """æ™ºèƒ½å¾ªç¯æ£€æµ‹å’Œä¼˜åŒ–æœºåˆ¶"""
        try:
            current_content = content

            for round_num in range(1, max_rounds + 1):
                self.logger.info(f"ğŸ” ç¬¬ {round_num}/{max_rounds} è½®AIæ£€æµ‹...")

                # æ‰§è¡ŒAIæ£€æµ‹
                detection_result = await detector.detect_ai_content(current_content)

                if not detection_result.success:
                    self.logger.error(f"âŒ ç¬¬ {round_num} è½®æ£€æµ‹å¤±è´¥: {detection_result.error}")
                    continue

                ai_probability = detection_result.ai_probability
                self.logger.info(f"ğŸ“Š ç¬¬ {round_num} è½®æ£€æµ‹ç»“æœ: AIæ¦‚ç‡ {ai_probability}%")

                # å¦‚æœé€šè¿‡æ£€æµ‹ï¼Œç›´æ¥è¿”å›
                if ai_probability < threshold:
                    self.logger.info(f"ğŸ‰ ç¬¬ {round_num} è½®æ£€æµ‹é€šè¿‡! (AIæ¦‚ç‡: {ai_probability}% < é˜ˆå€¼: {threshold}%)")
                    # æ›´æ–°æ–‡ç« å†…å®¹ä¸ºæœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬
                    if round_num > 1:  # å¦‚æœç»è¿‡äº†ä¼˜åŒ–
                        article.content_optimized = current_content
                        self.logger.info("ğŸ’¾ å·²æ›´æ–°æ–‡ç« ä¸ºæœ€ç»ˆä¼˜åŒ–ç‰ˆæœ¬")
                    return detection_result

                # å¦‚æœæ˜¯æœ€åä¸€è½®ï¼Œä¸å†ä¼˜åŒ–
                if round_num == max_rounds:
                    self.logger.warning(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§ä¼˜åŒ–è½®æ•° ({max_rounds})ï¼Œåœæ­¢ä¼˜åŒ–")
                    self.logger.warning(f"âš ï¸ æœ€ç»ˆAIæ¦‚ç‡: {ai_probability}% (æœªè¾¾åˆ°é˜ˆå€¼: {threshold}%)")
                    return detection_result

                # éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–
                self.logger.info(f"ğŸ”„ ç¬¬ {round_num} è½®æ£€æµ‹æœªé€šè¿‡ (AIæ¦‚ç‡: {ai_probability}% >= é˜ˆå€¼: {threshold}%)")
                self.logger.info(f"ğŸ› ï¸ å¼€å§‹ç¬¬ {round_num} è½®å†…å®¹ä¼˜åŒ–...")

                # æ‰§è¡Œé’ˆå¯¹æ€§ä¼˜åŒ–
                optimization_result = await self._optimize_for_ai_detection(
                    current_content, ai_probability, round_num
                )

                if optimization_result.success:
                    current_content = optimization_result.content
                    self.logger.info(f"âœ… ç¬¬ {round_num} è½®ä¼˜åŒ–å®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(current_content)} å­—ç¬¦")
                else:
                    self.logger.error(f"âŒ ç¬¬ {round_num} è½®ä¼˜åŒ–å¤±è´¥: {optimization_result.error}")
                    # ä¼˜åŒ–å¤±è´¥æ—¶è¿”å›å½“å‰æ£€æµ‹ç»“æœ
                    return detection_result

            # å¦‚æœæ‰€æœ‰è½®æ¬¡éƒ½å®Œæˆä½†ä»æœªé€šè¿‡ï¼Œè¿”å›æœ€åçš„æ£€æµ‹ç»“æœ
            return detection_result

        except Exception as e:
            self.logger.error(f"ğŸ’¥ æ™ºèƒ½å¾ªç¯æ£€æµ‹å¼‚å¸¸: {e}")
            # è¿”å›ä¸€ä¸ªå¤±è´¥çš„æ£€æµ‹ç»“æœ
            from .ai_detection import AIDetectionResult
            return AIDetectionResult(
                ai_probability=100.0,
                confidence=0.0,
                detector="zhuque",
                status="error",
                error=f"å¾ªç¯æ£€æµ‹å¼‚å¸¸: {str(e)}"
            )

    async def _optimize_for_ai_detection(self, content: str, current_ai_prob: float, round_num: int):
        """é’ˆå¯¹AIæ£€æµ‹ç»“æœè¿›è¡Œä¼˜åŒ–"""
        try:
            from .llm_api import get_llm_service
            from .prompt_manager import get_prompt_manager, ContentType

            llm_service = get_llm_service()
            prompt_manager = get_prompt_manager()

            # ä½¿ç”¨å¢å¼ºçš„æç¤ºè¯ç®¡ç†å™¨ç”Ÿæˆä¼˜åŒ–æç¤ºè¯
            self.logger.info(f"ğŸ¯ ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨ç”Ÿæˆç¬¬{round_num}è½®ä¼˜åŒ–æç¤ºè¯...")

            # å‡è®¾æ˜¯æŠ€æœ¯å†…å®¹ç±»å‹ï¼ˆå®é™…åº”ç”¨ä¸­å¯ä»¥ä¼ å…¥æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰
            content_type = ContentType.GENERAL

            optimization_prompt = prompt_manager.get_optimization_prompt(
                content=content,
                ai_probability=current_ai_prob,
                round_number=round_num,
                content_type=content_type,
                detection_feedback=f"å½“å‰AIæ¦‚ç‡ä¸º{current_ai_prob}%ï¼Œéœ€è¦é™ä½åˆ°25%ä»¥ä¸‹",
                platform="toutiao"
            )

            # æ˜¾ç¤ºä½¿ç”¨çš„ä¼˜åŒ–promptï¼ˆæˆªå–å‰400å­—ç¬¦ï¼‰
            prompt_preview = optimization_prompt[:400] + "..." if len(optimization_prompt) > 400 else optimization_prompt
            self.logger.info("ğŸ“ ä½¿ç”¨çš„ä¼˜åŒ–Prompt:")
            self.logger.info("â”€" * 60)
            self.logger.info(prompt_preview)
            self.logger.info("â”€" * 60)

            # æ‰§è¡Œä¼˜åŒ–
            result = await llm_service.optimize_content(
                content=content,
                custom_prompt=optimization_prompt
            )

            return result

        except Exception as e:
            self.logger.error(f"ğŸ’¥ é’ˆå¯¹æ€§ä¼˜åŒ–å¼‚å¸¸: {e}")
            from .llm_api import LLMResult
            return LLMResult(
                success=False,
                content=content,
                error=f"ä¼˜åŒ–å¼‚å¸¸: {str(e)}"
            )


# Global processor instance
_processor_instance = None


def get_article_processor() -> ArticleProcessor:
    """Get the global article processor instance."""
    global _processor_instance
    if _processor_instance is None:
        _processor_instance = ArticleProcessor()
    return _processor_instance
