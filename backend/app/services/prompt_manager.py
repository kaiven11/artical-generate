"""
Enhanced prompt management service for AI detection evasion.
"""

import logging
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from datetime import datetime
import random

from ..models.prompt import PromptTemplate, PromptType
from ..core.database import get_db_session, get_db_connection


class OptimizationLevel(str, Enum):
    """Optimization levels based on AI detection results."""
    LIGHT = "light"      # AI probability < 25%
    STANDARD = "standard"  # AI probability 25-50%
    HEAVY = "heavy"      # AI probability > 50%


class ContentType(str, Enum):
    """Content types for differentiated processing."""
    TECHNICAL = "technical"
    NEWS = "news"
    TUTORIAL = "tutorial"
    GENERAL = "general"


class PromptManager:
    """Enhanced prompt management service for AI detection evasion."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self._templates_cache = {}
        self._load_templates()
    
    def _load_templates(self):
        """Load prompt templates from database."""
        try:
            # Use direct database connection for synchronous loading
            from ..core.database import get_db_connection
            conn = get_db_connection()
            try:
                cursor = conn.cursor()
                cursor.execute("SELECT * FROM prompt_templates WHERE is_active = 1")
                rows = cursor.fetchall()

                for row in rows:
                    key = f"{row['type']}_{row['name']}"
                    # Create a simple template object
                    template = type('PromptTemplate', (), {
                        'id': row['id'],
                        'name': row['name'],
                        'type': row['type'],
                        'template': row['template'],
                        'is_active': row['is_active']
                    })()
                    self._templates_cache[key] = template

                self.logger.info(f"Loaded {len(rows)} prompt templates")
            finally:
                conn.close()
        except Exception as e:
            self.logger.error(f"Failed to load templates: {e}")
    
    def get_optimization_prompt(
        self,
        content: str,
        ai_probability: float,
        round_number: int = 1,
        content_type: ContentType = ContentType.GENERAL,
        detection_feedback: str = "",
        platform: str = "toutiao"
    ) -> str:
        """
        Get optimized prompt based on AI detection results and context.

        Args:
            content: Content to optimize
            ai_probability: Current AI detection probability
            round_number: Optimization round number
            content_type: Type of content
            detection_feedback: Feedback from AI detection
            platform: Target platform

        Returns:
            Optimized prompt string
        """
        # Determine optimization level
        if ai_probability > 50:
            level = OptimizationLevel.HEAVY
        elif ai_probability > 25:
            level = OptimizationLevel.STANDARD
        else:
            level = OptimizationLevel.LIGHT

        self.logger.info(f"ðŸŽ¯ é€‰æ‹©ä¼˜åŒ–çº§åˆ«: {level.value} (AIæ¦‚çŽ‡: {ai_probability}%, è½®æ¬¡: {round_number})")

        # Try to get prompt template from database first
        template = self.get_template_by_criteria(
            template_type=PromptType.OPTIMIZATION,
            optimization_level=level,
            content_type=content_type
        )

        if template:
            self.logger.info(f"ðŸ“š ä½¿ç”¨æ•°æ®åº“æç¤ºè¯æ¨¡æ¿: {template.name}")
            # Use template from database and fill variables
            prompt = self._fill_template_variables(
                template=template,
                variables={
                    'content': content,
                    'objective': self._get_optimization_objective(level, round_number),
                    'level_requirements': self._get_level_requirements_text(level),
                    'platform': platform,
                    'detection_feedback': detection_feedback if detection_feedback else "æ— ç‰¹æ®Šåé¦ˆ"
                }
            )
        else:
            self.logger.info("ðŸ“ æ•°æ®åº“ä¸­æœªæ‰¾åˆ°åˆé€‚æ¨¡æ¿ï¼Œä½¿ç”¨åŠ¨æ€æž„å»ºæç¤ºè¯")
            # Fallback to dynamic prompt building
            prompt = self._build_dynamic_prompt(
                content=content,
                level=level,
                round_number=round_number,
                content_type=content_type,
                detection_feedback=detection_feedback,
                platform=platform
            )

        return prompt
    
    def _build_dynamic_prompt(
        self,
        content: str,
        level: OptimizationLevel,
        round_number: int,
        content_type: ContentType,
        detection_feedback: str,
        platform: str
    ) -> str:
        """Build dynamic prompt based on parameters."""
        
        # Base role and objective
        role = self._get_role_definition(content_type)
        objective = self._get_optimization_objective(level, round_number)
        requirements = self._get_optimization_requirements(level, content_type, platform)
        
        # Build prompt
        prompt_parts = [
            f"ä½ æ˜¯{role}ã€‚",
            "",
            f"ä¼˜åŒ–ç›®æ ‡ï¼š{objective}",
            "",
            "å…·ä½“è¦æ±‚ï¼š",
        ]
        
        # Add requirements
        for i, req in enumerate(requirements, 1):
            prompt_parts.append(f"{i}. {req}")
        
        # Add detection feedback if available
        if detection_feedback:
            prompt_parts.extend([
                "",
                f"æ£€æµ‹åé¦ˆï¼š{detection_feedback}",
            ])
        
        # Add content and output instruction
        prompt_parts.extend([
            "",
            "åŽŸæ–‡å†…å®¹ï¼š",
            content,
            "",
            "è¯·ç›´æŽ¥è¾“å‡ºä¼˜åŒ–åŽçš„å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜Žã€‚"
        ])
        
        return "\n".join(prompt_parts)
    
    def _get_role_definition(self, content_type: ContentType) -> str:
        """Get role definition based on content type."""
        roles = {
            ContentType.TECHNICAL: "ä¸€ä½èµ„æ·±çš„æŠ€æœ¯å†…å®¹åˆ›ä½œè€…ï¼Œå…·æœ‰ä¸°å¯Œçš„æŠ€æœ¯å†™ä½œç»éªŒå’Œæ·±åŽšçš„è¡Œä¸šèƒŒæ™¯",
            ContentType.NEWS: "ä¸€ä½ç»éªŒä¸°å¯Œçš„æ–°é—»ç¼–è¾‘ï¼Œæ“…é•¿å°†ä¿¡æ¯è½¬åŒ–ä¸ºå¼•äººå…¥èƒœçš„æ–°é—»æŠ¥é“",
            ContentType.TUTORIAL: "ä¸€ä½ä¸“ä¸šçš„æ•™è‚²å†…å®¹åˆ›ä½œè€…ï¼Œå–„äºŽå°†å¤æ‚æ¦‚å¿µè½¬åŒ–ä¸ºæ˜“æ‡‚çš„æ•™ç¨‹",
            ContentType.GENERAL: "ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ï¼Œå…·æœ‰å¤šå¹´çš„å†™ä½œå’Œç¼–è¾‘ç»éªŒ"
        }
        return roles.get(content_type, roles[ContentType.GENERAL])
    
    def _get_optimization_objective(self, level: OptimizationLevel, round_number: int) -> str:
        """Get optimization objective based on level and round."""
        base_objectives = {
            OptimizationLevel.LIGHT: "å¯¹å†…å®¹è¿›è¡Œè½»åº¦ä¼˜åŒ–ï¼Œæå‡è‡ªç„¶åº¦å’Œå¯è¯»æ€§",
            OptimizationLevel.STANDARD: "å¯¹å†…å®¹è¿›è¡Œä¸­åº¦æ”¹å†™ï¼Œæ˜¾è‘—é™ä½ŽAIç—•è¿¹",
            OptimizationLevel.HEAVY: "å¯¹å†…å®¹è¿›è¡Œæ·±åº¦é‡æž„ï¼Œå½»åº•æ¶ˆé™¤AIç”Ÿæˆç‰¹å¾"
        }
        
        base = base_objectives[level]
        
        if round_number > 1:
            base += f"ï¼ˆç¬¬{round_number}è½®ä¼˜åŒ–ï¼Œéœ€è¦æ›´åŠ å½»åº•çš„æ”¹å†™ï¼‰"
        
        return base

    def _get_optimization_requirements(
        self,
        level: OptimizationLevel,
        content_type: ContentType,
        platform: str
    ) -> List[str]:
        """Get optimization requirements based on level, content type and platform."""

        # Base requirements for all levels
        base_requirements = [
            "ä¿æŒåŽŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯å®Œæ•´æ€§",
            "ç¡®ä¿å†…å®¹çš„é€»è¾‘ç»“æž„æ¸…æ™°åˆç†"
        ]

        # Level-specific requirements
        level_requirements = {
            OptimizationLevel.LIGHT: [
                "è°ƒæ•´éƒ¨åˆ†å¥å¼ç»“æž„ï¼Œå¢žåŠ è¡¨è¾¾çš„è‡ªç„¶æ€§",
                "é€‚å½“æ·»åŠ ä¸€äº›ä¸ªäººåŒ–çš„è¡¨è¾¾æ–¹å¼",
                "ä¿æŒåŽŸæ–‡çš„æ•´ä½“é£Žæ ¼å’Œè¯­è°ƒ"
            ],
            OptimizationLevel.STANDARD: [
                "æ˜¾è‘—æ”¹å˜å¥å¼ç»“æž„å’Œè¡¨è¾¾æ–¹å¼",
                "å¢žåŠ æ›´å¤šä¸ªäººåŒ–çš„è§è§£å’Œè¯„è®º",
                "è°ƒæ•´æ®µè½ç»„ç»‡ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»å†™ä½œä¹ æƒ¯",
                "æ·»åŠ é€‚å½“çš„å£è¯­åŒ–è¡¨è¾¾å’Œè¯­æ°”è¯"
            ],
            OptimizationLevel.HEAVY: [
                "å½»åº•é‡æž„æ–‡ç« çš„è¡¨è¾¾æ–¹å¼å’Œè¯­è¨€é£Žæ ¼",
                "å¤§é‡å¢žåŠ ä¸ªäººåŒ–çš„è§‚ç‚¹ã€ç»éªŒå’Œæ„Ÿå—",
                "å®Œå…¨æ”¹å˜å¥å¼æ¨¡å¼ï¼Œé¿å…AIå†™ä½œçš„è§„æ•´æ€§",
                "æ·»åŠ ä¸°å¯Œçš„æƒ…æ„Ÿè‰²å½©å’Œä¸»è§‚åˆ¤æ–­",
                "æ¨¡æ‹ŸçœŸå®žçš„äººç±»æ€ç»´è¿‡ç¨‹å’Œè¡¨è¾¾ä¹ æƒ¯",
                "å¢žåŠ ä¸è§„åˆ™çš„è¯­è¨€ç‰¹å¾å’Œä¸ªæ€§åŒ–è¡¨è¾¾"
            ]
        }

        # Content type specific requirements
        content_requirements = {
            ContentType.TECHNICAL: [
                "ä¿æŒæŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§",
                "æ·»åŠ ä¸ªäººçš„æŠ€æœ¯è§è§£å’Œå®žè·µç»éªŒ",
                "é€‚å½“åˆ†äº«ç›¸å…³çš„æŠ€æœ¯èƒŒæ™¯å’Œåº”ç”¨åœºæ™¯"
            ],
            ContentType.NEWS: [
                "å¢žåŠ æ–°é—»è¯„è®ºå’Œä¸ªäººè§‚ç‚¹",
                "æ·»åŠ å¯¹äº‹ä»¶çš„åˆ†æžå’Œé¢„æµ‹",
                "èžå…¥æ—¶äº‹èƒŒæ™¯å’Œç›¸å…³è”æƒ³"
            ],
            ContentType.TUTORIAL: [
                "å¢žåŠ ä¸ªäººçš„å­¦ä¹ å¿ƒå¾—å’Œå®žè·µå»ºè®®",
                "æ·»åŠ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆçš„åˆ†äº«",
                "èžå…¥æ•™å­¦ç»éªŒå’Œå­¦ä¹ æŠ€å·§"
            ],
            ContentType.GENERAL: [
                "å¢žåŠ ä¸ªäººçš„ç”Ÿæ´»æ„Ÿæ‚Ÿå’Œç»éªŒåˆ†äº«",
                "æ·»åŠ ç›¸å…³çš„è”æƒ³å’Œæ€è€ƒè¿‡ç¨‹"
            ]
        }

        # Platform specific requirements
        platform_requirements = {
            "toutiao": [
                "æ ‡é¢˜è¦æœ‰å¸å¼•åŠ›ï¼Œå†…å®¹è¦æœ‰è¯é¢˜æ€§",
                "é€‚åˆå¤§ä¼—é˜…è¯»ï¼Œè¯­è¨€é€šä¿—æ˜“æ‡‚",
                "å¢žåŠ äº’åŠ¨æ€§å’Œäº‰è®®æ€§å…ƒç´ "
            ],
            "weixin": [
                "å†…å®¹è¦æœ‰ä»·å€¼ï¼Œé€‚åˆåˆ†äº«ä¼ æ’­",
                "è¯­è¨€è¦ç²¾å‡†ï¼Œé€»è¾‘è¦æ¸…æ™°",
                "å¢žåŠ å®žç”¨æ€§å’Œå¯æ“ä½œæ€§"
            ],
            "zhihu": [
                "å†…å®¹è¦ä¸“ä¸šæ·±å…¥ï¼Œæœ‰çŸ¥è¯†ä»·å€¼",
                "é€»è¾‘è¦ä¸¥å¯†ï¼Œè®ºè¯è¦å……åˆ†",
                "å¢žåŠ ä¸“ä¸šæ€§å’Œæƒå¨æ€§"
            ]
        }

        # Combine all requirements
        requirements = base_requirements.copy()
        requirements.extend(level_requirements.get(level, []))
        requirements.extend(content_requirements.get(content_type, []))
        requirements.extend(platform_requirements.get(platform, []))

        return requirements

    def get_translation_prompt(
        self,
        content: str,
        source_lang: str = "en",
        target_lang: str = "zh",
        content_type: ContentType = ContentType.GENERAL,
        title: str = ""
    ) -> str:
        """Get translation prompt with humanization features."""

        role = self._get_role_definition(content_type)

        # Language mapping
        lang_names = {
            "en": "è‹±æ–‡",
            "zh": "ä¸­æ–‡",
            "ja": "æ—¥æ–‡",
            "ko": "éŸ©æ–‡"
        }

        source_name = lang_names.get(source_lang, source_lang)
        target_name = lang_names.get(target_lang, target_lang)

        prompt_parts = [
            f"ä½ æ˜¯{role}ï¼ŒåŒæ—¶ä¹Ÿæ˜¯ä¸€ä½ä¸“ä¸šçš„ç¿»è¯‘ä¸“å®¶ã€‚",
            "",
            f"è¯·å°†ä»¥ä¸‹{source_name}å†…å®¹ç¿»è¯‘æˆ{target_name}ï¼Œè¦æ±‚ï¼š",
            "",
            "1. ä¿æŒåŽŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œä¿¡æ¯å®Œæ•´æ€§",
            "2. ä½¿ç”¨è‡ªç„¶æµç•…çš„ä¸­æ–‡è¡¨è¾¾ï¼Œé¿å…ç¿»è¯‘è…”",
            "3. é€‚å½“è°ƒæ•´å¥å¼ä»¥ç¬¦åˆä¸­æ–‡é˜…è¯»ä¹ æƒ¯",
            "4. ä¿ç•™ä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§",
            "5. å¢žåŠ é€‚å½“çš„æœ¬åœŸåŒ–è¡¨è¾¾ï¼Œä½¿å…¶æ›´ç¬¦åˆä¸­æ–‡è¯­å¢ƒ",
            "6. ä¿æŒæ®µè½ç»“æž„ï¼Œä½†å¯ä»¥é€‚å½“è°ƒæ•´å¥å­ç»„ç»‡",
            ""
        ]

        if title:
            prompt_parts.extend([
                f"æ–‡ç« æ ‡é¢˜ï¼š{title}",
                ""
            ])

        prompt_parts.extend([
            "åŽŸæ–‡å†…å®¹ï¼š",
            content,
            "",
            "è¯·ç›´æŽ¥è¾“å‡ºç¿»è¯‘ç»“æžœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜Žã€‚"
        ])

        return "\n".join(prompt_parts)

    def create_enhanced_templates(self) -> List[PromptTemplate]:
        """Create enhanced prompt templates for AI detection evasion."""

        templates = []

        # Basic optimization template
        basic_optimization = PromptTemplate()
        basic_optimization.name = "basic_humanization_v2"
        basic_optimization.display_name = "åŸºç¡€äººæ€§åŒ–ä¼˜åŒ–æ¨¡æ¿ v2.0"
        basic_optimization.description = "åŸºç¡€çš„å†…å®¹äººæ€§åŒ–ä¼˜åŒ–ï¼Œé€‚ç”¨äºŽè½»åº¦AIç—•è¿¹é™ä½Ž"
        basic_optimization.template = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å†…å®¹åˆ›ä½œä¸“å®¶ï¼Œå…·æœ‰å¤šå¹´çš„å†™ä½œå’Œç¼–è¾‘ç»éªŒã€‚

ä¼˜åŒ–ç›®æ ‡ï¼šå¯¹å†…å®¹è¿›è¡Œè½»åº¦ä¼˜åŒ–ï¼Œæå‡è‡ªç„¶åº¦å’Œå¯è¯»æ€§

å…·ä½“è¦æ±‚ï¼š
1. ä¿æŒåŽŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯å®Œæ•´æ€§
2. ç¡®ä¿å†…å®¹çš„é€»è¾‘ç»“æž„æ¸…æ™°åˆç†
3. è°ƒæ•´éƒ¨åˆ†å¥å¼ç»“æž„ï¼Œå¢žåŠ è¡¨è¾¾çš„è‡ªç„¶æ€§
4. é€‚å½“æ·»åŠ ä¸€äº›ä¸ªäººåŒ–çš„è¡¨è¾¾æ–¹å¼
5. ä¿æŒåŽŸæ–‡çš„æ•´ä½“é£Žæ ¼å’Œè¯­è°ƒ

åŽŸæ–‡å†…å®¹ï¼š
{content}

è¯·ç›´æŽ¥è¾“å‡ºä¼˜åŒ–åŽçš„å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜Žã€‚"""
        basic_optimization.type = PromptType.OPTIMIZATION
        basic_optimization.variables = ["content"]
        basic_optimization.is_active = True
        basic_optimization.priority = 5
        templates.append(basic_optimization)

        # AI trace reduction template
        ai_reduction = PromptTemplate()
        ai_reduction.name = "ai_trace_reduction_v2"
        ai_reduction.display_name = "AIç—•è¿¹æ·±åº¦é™ä½Žæ¨¡æ¿ v2.0"
        ai_reduction.description = "ä¸“é—¨ç”¨äºŽé™ä½ŽAIæ£€æµ‹æµ“åº¦çš„æ·±åº¦ä¼˜åŒ–æ¨¡æ¿"
        ai_reduction.template = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å†…å®¹åˆ›ä½œè€…ï¼Œå…·æœ‰ä¸°å¯Œçš„å†™ä½œç»éªŒå’Œæ·±åŽšçš„è¡Œä¸šèƒŒæ™¯ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼šå¯¹å†…å®¹è¿›è¡Œæ·±åº¦é‡æž„ï¼Œå½»åº•æ¶ˆé™¤AIç”Ÿæˆç‰¹å¾

å…·ä½“è¦æ±‚ï¼š
1. ä¿æŒåŽŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯å®Œæ•´æ€§
2. å½»åº•é‡æž„æ–‡ç« çš„è¡¨è¾¾æ–¹å¼å’Œè¯­è¨€é£Žæ ¼
3. å¤§é‡å¢žåŠ ä¸ªäººåŒ–çš„è§‚ç‚¹ã€ç»éªŒå’Œæ„Ÿå—
4. å®Œå…¨æ”¹å˜å¥å¼æ¨¡å¼ï¼Œé¿å…AIå†™ä½œçš„è§„æ•´æ€§
5. æ·»åŠ ä¸°å¯Œçš„æƒ…æ„Ÿè‰²å½©å’Œä¸»è§‚åˆ¤æ–­
6. æ¨¡æ‹ŸçœŸå®žçš„äººç±»æ€ç»´è¿‡ç¨‹å’Œè¡¨è¾¾ä¹ æƒ¯
7. å¢žåŠ ä¸è§„åˆ™çš„è¯­è¨€ç‰¹å¾å’Œä¸ªæ€§åŒ–è¡¨è¾¾
8. é€‚å½“æ·»åŠ å£è¯­åŒ–è¡¨è¾¾å’Œè¯­æ°”è¯

{detection_feedback}

åŽŸæ–‡å†…å®¹ï¼š
{content}

è¯·ç›´æŽ¥è¾“å‡ºä¼˜åŒ–åŽçš„å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜Žã€‚"""
        ai_reduction.type = PromptType.OPTIMIZATION
        ai_reduction.variables = ["content", "detection_feedback"]
        ai_reduction.is_active = True
        ai_reduction.priority = 10
        templates.append(ai_reduction)

        # Technical content optimization
        technical_optimization = PromptTemplate()
        technical_optimization.name = "technical_humanization_v2"
        technical_optimization.display_name = "æŠ€æœ¯å†…å®¹äººæ€§åŒ–æ¨¡æ¿ v2.0"
        technical_optimization.description = "ä¸“é—¨ç”¨äºŽæŠ€æœ¯æ–‡ç« çš„äººæ€§åŒ–ä¼˜åŒ–"
        technical_optimization.template = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„æŠ€æœ¯å†…å®¹åˆ›ä½œè€…ï¼Œå…·æœ‰ä¸°å¯Œçš„æŠ€æœ¯å†™ä½œç»éªŒå’Œæ·±åŽšçš„è¡Œä¸šèƒŒæ™¯ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼š{objective}

å…·ä½“è¦æ±‚ï¼š
1. ä¿æŒåŽŸæ–‡çš„æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯å®Œæ•´æ€§
2. ä¿æŒæŠ€æœ¯æœ¯è¯­çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
3. æ·»åŠ ä¸ªäººçš„æŠ€æœ¯è§è§£å’Œå®žè·µç»éªŒ
4. é€‚å½“åˆ†äº«ç›¸å…³çš„æŠ€æœ¯èƒŒæ™¯å’Œåº”ç”¨åœºæ™¯
5. {level_requirements}
6. ç¡®ä¿å†…å®¹é€‚åˆ{platform}å¹³å°å‘å¸ƒ

{detection_feedback}

åŽŸæ–‡å†…å®¹ï¼š
{content}

è¯·ç›´æŽ¥è¾“å‡ºä¼˜åŒ–åŽçš„å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯´æ˜Žã€‚"""
        technical_optimization.type = PromptType.OPTIMIZATION
        technical_optimization.variables = ["content", "objective", "level_requirements", "platform", "detection_feedback"]
        technical_optimization.is_active = True
        technical_optimization.priority = 8
        templates.append(technical_optimization)

        return templates

    def save_templates_to_db(self, templates: List[PromptTemplate]):
        """Save templates to database."""
        try:
            with get_db_session() as session:
                for template in templates:
                    # Check if template already exists
                    existing = session.query(PromptTemplate).filter(
                        PromptTemplate.name == template.name
                    ).first()

                    if existing:
                        # Update existing template
                        for attr in ['display_name', 'description', 'template', 'variables', 'priority']:
                            setattr(existing, attr, getattr(template, attr))
                        existing.updated_at = datetime.utcnow()
                        self.logger.info(f"Updated template: {template.name}")
                    else:
                        # Add new template
                        session.add(template)
                        self.logger.info(f"Added new template: {template.name}")

                session.commit()
                self.logger.info(f"Successfully saved {len(templates)} templates")

                # Reload cache
                self._load_templates()

        except Exception as e:
            self.logger.error(f"Failed to save templates: {e}")
            raise

    def get_template_by_criteria(
        self,
        template_type: PromptType,
        optimization_level: OptimizationLevel = None,
        content_type: ContentType = None
    ) -> Optional[PromptTemplate]:
        """Get template by specific criteria."""

        # Define template selection logic
        template_mapping = {
            (PromptType.OPTIMIZATION, OptimizationLevel.LIGHT): "basic_humanization_v2",
            (PromptType.OPTIMIZATION, OptimizationLevel.STANDARD): "ai_trace_reduction_v2",
            (PromptType.OPTIMIZATION, OptimizationLevel.HEAVY): "ai_trace_reduction_v2",
        }

        # Special handling for technical content
        if content_type == ContentType.TECHNICAL:
            template_name = "technical_humanization_v2"
        else:
            key = (template_type, optimization_level)
            template_name = template_mapping.get(key)

        if template_name:
            template_key = f"{template_type}_{template_name}"
            return self._templates_cache.get(template_key)

        return None

    def _fill_template_variables(self, template, variables: Dict[str, str]) -> str:
        """Fill template variables with provided values."""
        try:
            # Get template content
            template_content = template.template

            # Replace variables in template
            for var_name, var_value in variables.items():
                placeholder = f"{{{var_name}}}"
                template_content = template_content.replace(placeholder, str(var_value))

            self.logger.info(f"âœ… æ¨¡æ¿å˜é‡å¡«å……å®Œæˆï¼Œå…±æ›¿æ¢ {len(variables)} ä¸ªå˜é‡")
            return template_content

        except Exception as e:
            self.logger.error(f"âŒ æ¨¡æ¿å˜é‡å¡«å……å¤±è´¥: {e}")
            # Return original template if variable filling fails
            return template.template

    def _get_level_requirements_text(self, level: OptimizationLevel) -> str:
        """Get level requirements as formatted text."""
        level_requirements = {
            OptimizationLevel.LIGHT: [
                "è°ƒæ•´éƒ¨åˆ†å¥å¼ç»“æž„ï¼Œå¢žåŠ è¡¨è¾¾çš„è‡ªç„¶æ€§",
                "é€‚å½“æ·»åŠ ä¸€äº›ä¸ªäººåŒ–çš„è¡¨è¾¾æ–¹å¼",
                "ä¿æŒåŽŸæ–‡çš„æ•´ä½“é£Žæ ¼å’Œè¯­è°ƒ"
            ],
            OptimizationLevel.STANDARD: [
                "æ˜¾è‘—æ”¹å˜å¥å¼ç»“æž„å’Œè¡¨è¾¾æ–¹å¼",
                "å¢žåŠ æ›´å¤šä¸ªäººåŒ–çš„è§è§£å’Œè¯„è®º",
                "è°ƒæ•´æ®µè½ç»„ç»‡ï¼Œä½¿å…¶æ›´ç¬¦åˆäººç±»å†™ä½œä¹ æƒ¯",
                "æ·»åŠ é€‚å½“çš„å£è¯­åŒ–è¡¨è¾¾å’Œè¯­æ°”è¯"
            ],
            OptimizationLevel.HEAVY: [
                "å½»åº•é‡æž„æ–‡ç« çš„è¡¨è¾¾æ–¹å¼å’Œè¯­è¨€é£Žæ ¼",
                "å¤§é‡å¢žåŠ ä¸ªäººåŒ–çš„è§‚ç‚¹ã€ç»éªŒå’Œæ„Ÿå—",
                "å®Œå…¨æ”¹å˜å¥å¼æ¨¡å¼ï¼Œé¿å…AIå†™ä½œçš„è§„æ•´æ€§",
                "æ·»åŠ ä¸°å¯Œçš„æƒ…æ„Ÿè‰²å½©å’Œä¸»è§‚åˆ¤æ–­",
                "æ¨¡æ‹ŸçœŸå®žçš„äººç±»æ€ç»´è¿‡ç¨‹å’Œè¡¨è¾¾ä¹ æƒ¯",
                "å¢žåŠ ä¸è§„åˆ™çš„è¯­è¨€ç‰¹å¾å’Œä¸ªæ€§åŒ–è¡¨è¾¾"
            ]
        }

        requirements = level_requirements.get(level, [])
        return "; ".join(requirements)


# Service instance
_prompt_manager = None

def get_prompt_manager() -> PromptManager:
    """Get prompt manager instance."""
    global _prompt_manager
    if _prompt_manager is None:
        _prompt_manager = PromptManager()
    return _prompt_manager
